{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the whole process from initial atomic structure to training, evaluation and prediction. \n",
    "\n",
    "It includes:\n",
    "1. Read input atomic structures (saved as extxyz files) and create descriptors and their derivatives.\n",
    "\n",
    "2. Read inputs and outputs into a Data object.\n",
    "\n",
    "3. Create tensorflow dataset for training.\n",
    "\n",
    "4. Train the potential, use it for prediction and save the trained model.\n",
    "\n",
    "5. Load the trained model for retraining or prediction.\n",
    "\n",
    "\n",
    "The code has been run with Tensorflow 2.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atomdnn\n",
    "\n",
    "# 'float64' is used for reading data and train by default\n",
    "atomdnn.data_type = 'float64'\n",
    "\n",
    "# force and stress are evaluated by default, \n",
    "# if one only need to compute potential energy, then set compute_force to false\n",
    "atomdnn.compute_force = True\n",
    "\n",
    "# default value is for converting ev/A^3 to GPa\n",
    "# note that: the predicted positive stress means tension and negative stress means compression\n",
    "stress_unit_convert = 160.2176 \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from atomdnn import data\n",
    "from atomdnn.data import Data\n",
    "from atomdnn.data import *\n",
    "from atomdnn.io import *\n",
    "from atomdnn import network\n",
    "from atomdnn.network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Read input atomic structures (saved as extxyz files) and create descriptors and their derivatives**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor = {'name': 'acsf', \n",
    "              'cutoff': 6.5,\n",
    "              'etaG2':[0.01,0.025,0.05,0.075,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.8,1,1.5,2,3,5,10], \n",
    "              'etaG4': [0.01], \n",
    "              'zeta': [0.08,0.1,0.15,0.2,0.3,0.35,0.5,0.6,0.8,1.,1.5,2.,3.0,4.,5.5,7.0,10.0,25.0,50.0,100.0],\n",
    "              'lambda': [1.0, -1.0]}\n",
    "\n",
    "# define lammps excutable (serial or mpi) \n",
    "# LAMMPS has to be compiled with the added compute and dump_local subrutines (inside atomdnn/lammps)\n",
    "lmpexe = 'lmp_serial' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating fingerprints ...\n",
      "  so far finished for 5 images ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/bash: lmp_serial: command not found\n",
      "/bin/bash: lmp_serial: command not found\n",
      "/bin/bash: lmp_serial: command not found\n",
      "/bin/bash: lmp_serial: command not found\n",
      "/bin/bash: lmp_serial: command not found\n",
      "/bin/bash: lmp_serial: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  so far finished for 10 images ...\n",
      "Finish creating descriptors and their derivatives from total 10 images.\n",
      "It took 0.42 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/bash: lmp_serial: command not found\n",
      "/bin/bash: lmp_serial: command not found\n",
      "/bin/bash: lmp_serial: command not found\n",
      "/bin/bash: lmp_serial: command not found\n"
     ]
    }
   ],
   "source": [
    "xyzfile_path = './extxyz' \n",
    "xyzfile_name = 'example_extxyz.*' # a serials of files like example_extxyz.1, example_extxyz.2, ...example_extxyz.n\n",
    "descriptors_path = './descriptors'\n",
    "descriptor_filename = 'dump_fp' # a serials of dump_fp.* files will be created\n",
    "der_filename ='dump_der'\n",
    "\n",
    "# this will create a serials of files for descriptors and their derivatives inside descriptors_path\n",
    "# by default, descriptor files are saved as 'dump_fp.*' and derivatives are saved as 'dump_der.*'\n",
    "create_descriptors(xyzfile_path = xyzfile_path, \\\n",
    "                   xyzfile_name = xyzfile_name, \\\n",
    "                   lmpexe = lmpexe, \\\n",
    "                   descriptors_path = descriptors_path, \\\n",
    "                   descriptor = descriptor, \\\n",
    "                   descriptor_filename = descriptor_filename, \\\n",
    "                   der_filename = der_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Read inputs and outputs into a Data object** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading fingerprints data from LAMMPS dump files ./descriptors/dump_fp.i\n",
      "  Finish reading fingerprints from total 10 images.\n",
      "\n",
      "  image number = 10\n",
      "  max number of atom = 4\n",
      "  number of fingerprints = 59\n",
      "  type of atoms = 1\n",
      "\n",
      "Reading derivative data from a series of files ./descriptors/dump_der.i\n",
      "This may take a while ...\n",
      "  Finish reading dGdr derivatives from total 10 images.\n",
      "\n",
      "  Pad zeros to derivatives data if needed ...\n",
      "  Pading finished: 9 images derivatives have been padded with zeros.\n",
      "\n",
      "  image number = 10\n",
      "  max number of blocks = 150\n",
      "  number of fingerprints = 59\n",
      "\n",
      "  It took 0.34 seconds to read the derivatives data.\n"
     ]
    }
   ],
   "source": [
    "# create a Data object\n",
    "grdata = Data()\n",
    "\n",
    "# read inputs: descriptors and their derivatives\n",
    "fp_filename = descriptors_path + '/dump_fp.*'\n",
    "der_filename = descriptors_path + '/dump_der.*'\n",
    "\n",
    "grdata.read_inputdata(fp_filename = fp_filename,der_filename = der_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading outputs from extxyz files ...\n",
      "  Finish reading outputs from total 10 images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read outputs: potential energy, force and stress from extxyz files\n",
    "grdata.read_outputdata(xyzfile_path=xyzfile_path, xyzfile_name=xyzfile_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Create tensorflow dataset for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion may take a while for large datasets...\n",
      "It took 0.0923 second.\n"
     ]
    }
   ],
   "source": [
    "# convert data to tensors\n",
    "grdata.convert_data_to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensorflow dataset\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((grdata.input_dict,grdata.output_dict))\n",
    "\n",
    "# save the dataset\n",
    "dataset_name = './temp_tfdataset'\n",
    "tf.data.experimental.save(tf_dataset, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The above three steps just need to be done once for one data set, the training only uses the saved tensorflow dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Load the dataset and train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load() missing 1 required positional argument: 'element_spec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b1baf968f653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# The process of reading data and creating dataset is discussed in 'Data pipeline' section.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'example_tfdataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: load() missing 1 required positional argument: 'element_spec'"
     ]
    }
   ],
   "source": [
    "# load tensorflow dataset, for Tensorflow version lower than 2.6, need to specify element_spec.\n",
    "# The process of reading data and creating dataset is discussed in 'Data pipeline' section.\n",
    "\n",
    "dataset = tf.data.experimental.load('example_tfdataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-18bc7fec053e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# split the data to training, validation and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# split the data to training, validation and testing sets\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset,0.7,0.2,0.1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the network\n",
    "# See section 'Training' for detailed description on Network object.\n",
    "\n",
    "elements = ['C']\n",
    "act_fun = 'relu' # activation function\n",
    "nfp = get_fingerprints_num(dataset) # number of fingerprints (or descriptors)\n",
    "arch = [30,30] # NN layers\n",
    "\n",
    "model = Network(elements = elements,\\\n",
    "                num_fingerprints = nfp,\\\n",
    "                arch = arch,\\\n",
    "                activation_function = act_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "\n",
    "opt = 'Adam' # optimizer\n",
    "loss_fun = 'rmse' # loss function\n",
    "scaling = 'std' # scaling the traning data with standardization\n",
    "lr = 0.02 # learning rate\n",
    "loss_weights = {'pe' : 1, 'force' : 1, 'stress': 0.1} # the weights in loss function\n",
    "\n",
    "model.train(train_dataset, val_dataset, \\\n",
    "            optimizer=opt, \\\n",
    "            loss_fun = loss_fun, \\\n",
    "            batch_size=30, \\\n",
    "            lr=lr, \\\n",
    "            epochs=30, \\\n",
    "            scaling=scaling, \\\n",
    "            loss_weights=loss_weights, \\\n",
    "            compute_all_loss=True, \\\n",
    "            shuffle=True, \\\n",
    "            append_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "\n",
    "model.plot_loss(start_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using the first 5 data in test dataset\n",
    "\n",
    "model.evaluate(test_dataset.take(5),return_prediction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction using the first 5 data in test dataset\n",
    "\n",
    "input_dict = get_input_dict(test_dataset.take(5))\n",
    "model.predict(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model\n",
    "\n",
    "# we re-write the descriptor here to empasize that it should be the same one defined above\n",
    "descriptor = {'name': 'acsf', \n",
    "              'cutoff': 6.5001,\n",
    "              'etaG2':[0.01,0.025,0.05,0.075,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.8,1,1.5,2,3,5,10], \n",
    "              'etaG4': [0.01], \n",
    "              'zeta': [0.08,0.1,0.15,0.2,0.3,0.35,0.5,0.6,0.8,1.,1.5,2.,3.0,4.,5.5,7.0,10.0,25.0,50.0,100.0],\n",
    "              'lambda': [1.0, -1.0]}\n",
    "\n",
    "save_dir = 'example.tfdnn'\n",
    "network.save(model,save_dir,descriptor=descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Load the trained model for continuous training and prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_model = network.load(save_dir)\n",
    "\n",
    "# Re-train the model \n",
    "loss_weights = {'pe' : 1, 'force' : 1, 'stress': 0.1}\n",
    "\n",
    "opt = 'Adam'\n",
    "loss_fun = 'rmse'\n",
    "scaling = 'std'\n",
    "\n",
    "model.train(train_dataset, val_dataset, \\\n",
    "            optimizer=opt, \\\n",
    "            loss_fun = loss_fun, \\\n",
    "            batch_size=30, \\\n",
    "            lr=0.02, \\\n",
    "            epochs=5, \\\n",
    "            scaling=scaling, \\\n",
    "            loss_weights=loss_weights, \\\n",
    "            compute_all_loss=True, \\\n",
    "            shuffle=True, \\\n",
    "            append_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_model.evaluate(test_dataset.take(5),return_prediction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = get_input_dict(test_dataset.take(5))\n",
    "imported_model.predict(input_dict)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
