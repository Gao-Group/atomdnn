{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TFAtomDNN\n",
    "\n",
    "# 'float32' is used for reading data and train by default, one can set data_type to 'float64' here\n",
    "TFAtomDNN.data_type = 'float64'\n",
    "\n",
    "# force and stress are evaluated by default, \n",
    "# if one only need to compute potential energy, then set compute_force to false\n",
    "TFAtomDNN.compute_force = True\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from TFAtomDNN import data\n",
    "from TFAtomDNN import network\n",
    "from TFAtomDNN.data import Data\n",
    "from TFAtomDNN.data import *\n",
    "from TFAtomDNN.network import Network\n",
    "# import importlib\n",
    "# importlib.reload(TFAtomDNN.data)\n",
    "# importlib.reload(TFAtomDNN.network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read inputdata from LAMMPS dump files\n",
    "\n",
    "\n",
    "**read_inputdata(fp_filename=None, der_filename=None, image_num=None, read_der=TFAtomDNN.compute_force)** \n",
    "\n",
    "- **fp_filename**: fingerprints file path, use wildcard * in the name for a serials of files.\n",
    "\n",
    "- **fp_filename**: derivatives file name, use wildcard * in the name for a serials of files.\n",
    "\n",
    "- **image_num**: if not set, read all images\n",
    "\n",
    "- **read_der**: set to true if read derivative data\n",
    "\n",
    "\n",
    "It will generate input data in the form of tensorflow tensors, which can be accessed using keys:\n",
    "\n",
    "- **input_dict [ 'fingerprints' ] [ i ] [ j ] [ k ]** gives the k-th fingerprint of j-th atom in i-th image.\n",
    "    \n",
    "- **input_dict [ 'atom_type' ] [ i ] [ j ]** gives the atom type of j-th atom in i-th image.\n",
    "    \n",
    "- **input_dict [ 'dGdr' ] [ i ] [ j ] [ k ] [ m ]** gives the derivative of k-th fingerprint in j-th derivative data blcok of i-th image w.r.t m-th cooridinate.\n",
    "    \n",
    "- **input_dict [ 'center_atom_id' ] [ i ] [ j ]** gives the center atom id in j-th derivative data block of i-th image.\n",
    "    \n",
    "- **input_dict [ 'neighbor_atom_id' ] [ i ] [ j ]** gives the neighbor atom id in j-th derivative data block of i-th image. Note that the neighbors could be ghost atoms.\n",
    "    \n",
    "- **input_dict [ 'neighbor_atom_coord' ] [ i ] [ j ] [ m ] [ 0 ]** gives the m-th coordiate of neighbor atom in j-th derivative block of i-th image. Note that the last dimension of the array is 1 which is added for matrix multiplication in neural network force_stress_layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grdata = Data()\n",
    "fp_filename = 'data_graphene/dump_fp.*'\n",
    "der_filename = 'data_graphene/dump_der.*'\n",
    "grdata.read_inputdata(fp_filename=fp_filename,der_filename=der_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read output data genrated using LAMMPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_stress_filename = 'data_graphene/output.txt'\n",
    "force_filename = 'data_graphene/dump.force'\n",
    "grdata.read_outputdata_from_lmp(pe_stress_filename=pe_stress_filename,force_filename=force_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the data class to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"grdata.pickle\", \"wb\")\n",
    "pickle.dump(grdata, file, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data class from saved pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grdata = pickle.load(open(\"grdata.pickle\", \"rb\", -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffel and then split the data into training, validation and test sets\n",
    "\n",
    "**split(self, train_data_percent=None, val_data_percent=None, test_data_percent=None, data_size=None)**\n",
    "\n",
    "- **train_pct**: percentage of data used for training\n",
    "\n",
    "- **val_pct**: percentage of data used for validation\n",
    "\n",
    "- **test_pct**: percentage of data used for test\n",
    "\n",
    "- **data_size**: if not set, use the whole data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grdata.shuffel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_val,y_val),(x_test,y_test) = grdata.split(0.7,0.2,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Network object from class Network \n",
    "\n",
    "__init__(self, elements=None, num_fingerprints=None, arch=None,activation_function=None, data_type=None, import_dir=None)\n",
    "\n",
    "- **elements:** element list, required\n",
    "\n",
    "- **num_fingerprints:** number of fingerprints in data, required\n",
    "\n",
    "- **std**: = [mean, standard_deviation] of fingerprints, if set, standarlize the fingprints\n",
    "\n",
    "- **norm**: = [min, max] of fingerprints, if set, normalize the fingerprints\n",
    "\n",
    "- **arch:** number of layers of neural network\n",
    "\n",
    "- **activation_function:** if not set, default is 'tanh'\n",
    "\n",
    "- **import_dir:** read from the directory of a saved (imported) network, if used, all other parameters are disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(elements=['C'],num_fingerprints=grdata.num_fingerprints, std = [grdata.mean_fp,grdata.dev_fp],\n",
    "               arch=[30,30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "**train(self, train_input_dict, train_output_dict,\n",
    "              batch_size=None, epochs=None, loss_fn=None, optimizer=None, lr=None, train_force=False, train_stress=False)**\n",
    "\n",
    "- **train_input_dict**: input dictionary generated from build_dataset() for training\n",
    "    \n",
    "- **train_output_dict**: output dictionary generated from build_dataset() for training\n",
    "    \n",
    "- **batch_size**: if not set, use 30\n",
    "    \n",
    "- **epochs**: if not set, use 1\n",
    "    \n",
    "- **opimizer**: if not set, use Adam\n",
    "    \n",
    "- **lr**: learning rate, if not set, use 0.01\n",
    "    \n",
    "- **train_force**: if force used for training\n",
    "    \n",
    "- **train_stress**: if stress used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(x_train,y_train, validation_data=[x_val,y_val], batch_size=30, epochs=500,train_force=True,pe_loss_weight=0.01, force_loss_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grdata.input_dict['dGdr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grdata.input_dict['fingerprints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul(1/grdata.dev_fp*grdata.input_dict['dGdr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction: compute potential energy, force and stress\n",
    "\n",
    "**predict (self, input_dict, training=False,compute_force=True)**\n",
    "\n",
    "- **input_dict**: input dictionary generated from build_dataset function\n",
    "    \n",
    "- **training**: set to False\n",
    "    \n",
    "- **compute_force**: if compute force, derivative data are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_predict = tf.convert_to_tensor(stress_predict)\n",
    "mask = [True,True,True,False,True,True,False,False,True]\n",
    "tf.reshape(tf.boolean_mask(stress_predict, mask,axis=1),[-1,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.get('mae')\n",
    "\n",
    "pe_predict = model.predict(x_test)['pe']\n",
    "force_predict = model.predict(x_test)['force']\n",
    "stress_predict = model.predict(x_test)['stress']\n",
    "\n",
    "print(loss_fn(pe_predict,y_test['pe']))\n",
    "print(tf.reduce_mean(loss_fn(force_predict,y_test['force'])))\n",
    "print(tf.reduce_mean(loss_fn(stress_predict,y_test['stress'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__call__(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save trained model\n",
    "\n",
    "**save(obj, model_dir, descriptor=None)**\n",
    "\n",
    "- **obj**: Network object\n",
    "\n",
    "- **model_dir**: directory for saving the trained model\n",
    "\n",
    "- **descriptor**: descriptor parameters used to generate fingerprints, if set, a parameters file is generated for LAMMPS simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor = {'name': 'acsf', \n",
    "              'cutoff': 6.5001,\n",
    "              'etaG2':[0.01,0.025,0.05,0.075,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.8,1,1.5,2,3,5,10], \n",
    "              'etaG4': [0.01], \n",
    "              'zeta': [0.08,0.1,0.15,0.2,0.3,0.35,0.5,0.6,0.8,1.,1.5,2.,3.0,4.,5.5,7.0,10.0,25.0,50.0,100.0],\n",
    "              'lambda': [1.0, -1.0]}\n",
    "\n",
    "save_dir = 'graphene_24atoms.tfdnn'\n",
    "network.save(model, save_dir,descriptor=descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the saved model for continuous training and prediction\n",
    "\n",
    "**load(model_dir)**\n",
    "\n",
    "- **model_dir**: saved model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'graphene_24atoms.tfdnn'\n",
    "model = network.load(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print signature\n",
    "\n",
    "network.print_signature(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedata = data.read_inputdata_from_lmp(batch_mode=False, fp_filename='data_graphene_96atoms/dump_fingerprints.200',der_filename='data_graphene_96atoms/dump_fingerprints_der.200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peratom potential energy\n",
    "\n",
    "new_model.__call__(onedata.input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute peratom stress \n",
    "\n",
    "centerid = onedata.input_dict['center_atom_id']\n",
    "\n",
    "center_one_hot = tf.one_hot(centerid,depth=onedata.num_blocks,axis=1,dtype=onedata.data_type)\n",
    "\n",
    "stress_block = new_model.__call__(onedata.input_dict)['stress']\n",
    "\n",
    "stress_peratom = tf.matmul(center_one_hot,stress_block)\n",
    "\n",
    "evA2bar = 1602176\n",
    "\n",
    "for i in range(0,onedata.num_atoms):\n",
    "    print(i+1,\"          \",stress_peratom[0][i].numpy()*ev2bar)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_pe = new_model.__call__(onedata.input_dict)['atom_pe'][0]\n",
    "\n",
    "for i in range(0,onedata.num_atoms):\n",
    "    print(\"%d:   %.6g %.6 %.6 %.6 %.6 %.6 %.6 %.6\" % (i+1, atom_pe[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress = new_model.__call__(onedata.input_dict)['stress'][0]\n",
    "\n",
    "for i in range(0,onedata.num_atoms):\n",
    "    print(\"%d:   %.6g\" % (i+1, atom_pe[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.predict(onedata.input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force = new_model.predict(onedata.input_dict)['force'][0]\n",
    "\n",
    "print (\"%s %5s %15s %15s\"%(\"atom_id\",\"f_x\",\"f_y\",\"f_z\"))\n",
    "for i in range(0,onedata.num_atoms):\n",
    "    print(\"%d %15.6f %15.6f %15.6f\" % (i+1,force[i][0].numpy(), force[i][1].numpy(), force[i][2].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue the training \n",
    "\n",
    "new_model.train(grdata.train_input_dict, grdata.train_output_dict, batch_size=30, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.predict(grdata.test_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check C_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose the second image to test\n",
    "image = data.slice_dict (grdata.input_dict,0,1)\n",
    "model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image['neighbor_atom_id'][0][23])\n",
    "print(image['neighbor_atom_coord'][0][23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__call__(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../c_inference/inference_energy \"../example/graphene_energy.tfdnn/\" \"../example/data_graphene_96atoms/dump_fingerprints.200\" 96  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdataset = tf.data.Dataset.from_tensor_slices((grdata.input_dict,grdata.output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(tfdataset, 'graphene_tfdataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdataset = tf.data.experimental.load('graphene_tfdataset',element_spec=tfdataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debuging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_model = network.load('graphene_energy.tfdnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_model.predict(onedata.input_dict,compute_force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedata = data.read_inputdata_from_lmp(batch_mode=False, fp_filename='data_graphene_96atoms/dump_fingerprints.200',der_filename='data_graphene_96atoms/dump_fingerprints_der.200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(float('2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(onedata.input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(96):\n",
    "    print(i+1, model.predict(onedata.input_dict)['force'][0][i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedata.input_dict['fingerprints'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%.8e'%onedata.input_dict['fingerprints'][0][1][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
