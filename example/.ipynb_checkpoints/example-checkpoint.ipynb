{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the whole process from initial atomic structure to training, evaluation and prediction. It includes:\n",
    "\n",
    "\n",
    "1. Read input atomic structures (saved as extxyz files) and create descriptors and their derivatives.\n",
    "\n",
    "2. Read inputs and outputs into a Data object.\n",
    "\n",
    "3. Create tensorflow dataset for training.\n",
    "\n",
    "4. Train the potential and apply it for prediction.\n",
    "\n",
    "5. Save the trained model and then load it for retraining or prediction.\n",
    "\n",
    "\n",
    "The code has been tested on Tensorflow 2.5 and 2.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atomdnn\n",
    "\n",
    "# 'float64' is used for reading data and train by default\n",
    "atomdnn.data_type = 'float64'\n",
    "\n",
    "# force and stress are evaluated by default, \n",
    "# if one only need to compute potential energy, then set compute_force to false\n",
    "atomdnn.compute_force = True\n",
    "\n",
    "# default value is for converting ev/A^3 to GPa\n",
    "# note that: the predicted positive stress means tension and negative stress means compression\n",
    "stress_unit_convert = 160.2176 \n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from atomdnn import data\n",
    "from atomdnn.data import Data\n",
    "from atomdnn.data import *\n",
    "from atomdnn.descriptor import *\n",
    "from atomdnn import network\n",
    "from atomdnn.network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read input atomic structures (saved as extxyz files) and create descriptors and their derivatives**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of fingerprints = 14\n"
     ]
    }
   ],
   "source": [
    "descriptor = {'name': 'acsf', \n",
    "              'cutoff': 6.5,\n",
    "              'etaG2':[0.01,0.1,1,5,10], \n",
    "              'etaG4': [0.01], \n",
    "              'zeta': [0.08,1.0,10.0,100.0],\n",
    "              'lambda': [1.0, -1.0]}\n",
    "\n",
    "\n",
    "# define lammps excutable (serial or mpi) \n",
    "# LAMMPS has to be compiled with the added compute and dump_local subrutines (inside atomdnn/lammps)\n",
    "lmpexe = 'lmp_serial' \n",
    "#lmpexe = 'mpirun -np 2 lmp_mpi'  # can be mpi version\n",
    "\n",
    "\n",
    "elements = ['C']\n",
    "xyzfile_path='extxyz'\n",
    "xyzfile_name = 'example_extxyz.*' # a serials of files like example_extxyz.1, example_extxyz.2, ...example_extxyz.n\n",
    "descriptors_path = './descriptors'\n",
    "\n",
    "descriptor_filename = 'dump_fp.*' # a serials of dump_fp.* files will be created\n",
    "der_filename ='dump_der.*'\n",
    "\n",
    "print('total number of fingerprints = %i'%get_num_fingerprints(descriptor,elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating fingerprints and derivatives for 50 files ...\n",
      "  so far finished for 10 images ...\n",
      "  so far finished for 20 images ...\n",
      "  so far finished for 30 images ...\n",
      "  so far finished for 40 images ...\n",
      "  so far finished for 50 images ...\n",
      "Finish creating descriptors and derivatives for total 50 images.\n",
      "It took 11.06 seconds.\n"
     ]
    }
   ],
   "source": [
    "# this will create a serials of files for descriptors and their derivatives inside descriptors_path\n",
    "create_descriptors(lmpexe,\n",
    "                   elements,\n",
    "                   xyzfile_path, \n",
    "                   xyzfile_name, \n",
    "                   descriptors_path, \n",
    "                   descriptor, \n",
    "                   descriptor_filename, \n",
    "                   der_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read inputs&outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read inputs and outputs into a Data object** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Data object\n",
    "mydata = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading fingerprints from 'dump_fp.*' for total 50 files ...\n",
      "  so far read 50 images ...\n",
      "  Finish reading fingerprints from total 50 images.\n",
      "\n",
      "\n",
      "Start reading derivatives from 'dump_der.*' for total 50 files ...\n",
      "  This may take a while for large data set ...\n",
      "  so far read 50 images ...\n",
      "  Finish reading dGdr derivatives from total 50 images.\n",
      "\n",
      "  It took 1.22 seconds to read the derivatives data.\n",
      "\n",
      "---------- input dataset information ----------\n",
      "total images = 50\n",
      "max number of atoms = 4\n",
      "number of fingerprints = 14\n",
      "number of atom types = 1\n",
      "max number of derivative pairs = 200\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# read inputs: descriptors and their derivatives\n",
    "mydata.read_inputdata(descriptors_path, descriptor_filename, der_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading outputs from extxyz files ...\n",
      "  so far read 50 images ...\n",
      "  Finish reading outputs from total 50 images.\n",
      "\n",
      "\n",
      "---------- output dataset information ------------\n",
      "total images = 50\n",
      "max number of atoms = 4\n",
      "read_force = True\n",
      "read_stress = True\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# read outputs: potential energy, force and stress from extxyz files\n",
    "mydata.read_outputdata(xyzfile_path, xyzfile_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TFdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create tensorflow dataset for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion may take a while for large datasets...\n",
      "It took 0.0074 second.\n"
     ]
    }
   ],
   "source": [
    "# convert data to tensors\n",
    "mydata.convert_data_to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensorflow dataset\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((mydata.input_dict,mydata.output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './example_tfdata'\n",
    "#dataset_path = './mote2_tfdata'\n",
    "# save the dataset\n",
    "tf.data.experimental.save(tf_dataset, dataset_path)\n",
    "\n",
    "# save the element_spec to disk for future loading, this is only needed for tensorflow lower than 2.6\n",
    "with open(dataset_path + '/element_spec', 'wb') as out_: \n",
    "    pickle.dump(tf_dataset.element_spec, out_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The above three steps just need to be done once for one data set, the training only uses the saved tensorflow dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the dataset and train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tensorflow dataset, for Tensorflow version lower than 2.6, need to specify element_spec.\n",
    "\n",
    "with open(dataset_path + '/element_spec', 'rb') as in_:\n",
    "    element_spec = pickle.load(in_)\n",
    "\n",
    "dataset = tf.data.experimental.load(dataset_path,element_spec=element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning data: 35 images\n",
      "Validation data: 10 images\n",
      "Test data: 5 images\n"
     ]
    }
   ],
   "source": [
    "# split the data to training, validation and testing sets\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset,0.7,0.2,0.1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the network\n",
    "# See section 'Training' for detailed description on Network object.\n",
    "\n",
    "act_fun = 'relu' # activation function\n",
    "nfp = get_num_fingerprints(descriptor,elements) # number of fingerprints (or descriptors) from dataset\n",
    "arch = [10,10] # NN layers\n",
    "\n",
    "model = Network(elements, nfp, arch, act_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "\n",
    "opt = 'Adam' # optimizer\n",
    "loss_fun = 'mae' # loss function\n",
    "scaling = None # scaling the traning data with standardization\n",
    "lr = 0.01 # learning rate\n",
    "loss_weights = {'pe' : 1, 'force' : 0, 'stress': 0} # the weights in loss function\n",
    "\n",
    "model.train(train_dataset,\\\n",
    "            optimizer=opt, \\\n",
    "            loss_fun = loss_fun, \\\n",
    "            batch_size=30, \\\n",
    "            lr=lr, \\\n",
    "            epochs=5, \\\n",
    "            scaling=scaling, \\\n",
    "            loss_weights=loss_weights, \\\n",
    "            compute_all_loss=True, \\\n",
    "            shuffle=False, \\\n",
    "            append_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "\n",
    "model.plot_loss(start_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using the first 5 data in test dataset\n",
    "\n",
    "model.evaluate(test_dataset.take(5),return_prediction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction using the first 5 data in test dataset\n",
    "\n",
    "input_dict = get_input_dict(dataset.take(1))\n",
    "model.predict(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**save the trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we re-write the descriptor here to empasize that it should be the same one defined above\n",
    "descriptor = {'name': 'acsf', \n",
    "              'cutoff': 6.5,\n",
    "              'etaG2':[0.01,0.05,0.1,0.5,1,5,10], \n",
    "              'etaG4': [0.01], \n",
    "              'zeta': [0.08,0.2,1.0,5.0,10.0,50.0,100.0],\n",
    "              'lambda': [1.0, -1.0]}\n",
    "\n",
    "save_dir = 'example.tfdnn'\n",
    "network.save(model,save_dir,descriptor=descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the trained model for continuous training and prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_model = network.load(save_dir)\n",
    "\n",
    "# Re-train the model \n",
    "loss_weights = {'pe' : 1, 'force' : 1, 'stress': 0.1}\n",
    "\n",
    "opt = 'Adam'\n",
    "loss_fun = 'rmse'\n",
    "scaling = 'std'\n",
    "\n",
    "model.train(train_dataset, val_dataset, \n",
    "            optimizer=opt, \n",
    "            loss_fun = loss_fun, \n",
    "            batch_size=30, \n",
    "            lr=0.02, \n",
    "            epochs=5, \n",
    "            scaling=scaling, \n",
    "            loss_weights=loss_weights, \n",
    "            compute_all_loss=True, \n",
    "            shuffle=True, \n",
    "            append_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_model.evaluate(test_dataset.take(5),return_prediction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = get_input_dict(test_dataset.take(5))\n",
    "imported_model.predict(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras layer for energy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "fingerprint_num = get_num_fingerprints(descriptor,elements)\n",
    "\n",
    "zero_initializer = tf.keras.initializers.Zeros()\n",
    "one_initializer = tf.keras.initializers.Ones()\n",
    "\n",
    "# build keras DNN model \n",
    "neuron_number=[10,10]\n",
    "\n",
    "input={}\n",
    "atom_energy_layer={}\n",
    "hidden_layer={}\n",
    "activation_function=\"relu\"\n",
    "\n",
    "for i in range(len(elements)):\n",
    "    input[i] = keras.layers.Input(shape=[None,fingerprint_num], name=\"element_\"+elements[i]+\"_input\")\n",
    "    hidden_layer[i]={}\n",
    "    hidden_layer[i][0] = keras.layers.Dense(neuron_number[0], \n",
    "                                            kernel_initializer=one_initializer, \n",
    "                                            bias_initializer=zero_initializer, \n",
    "                                            activation=activation_function,\n",
    "                                            name=\"element_\"+elements[i]+\"_hidden1\")(input[i])\n",
    "    for j in range(1,len(neuron_number)):\n",
    "        hidden_layer[i][j] = keras.layers.Dense(neuron_number[j], \n",
    "                                                kernel_initializer=one_initializer, \n",
    "                                                bias_initializer=zero_initializer, \n",
    "                                                activation=activation_function,\n",
    "                                                name=\"element_\"+elements[i]+\"_hidden\"+str(j+1))(hidden_layer[i][j-1])\n",
    "        \n",
    "    atom_energy_layer[i] = keras.layers.Dense(1, kernel_initializer=one_initializer, \n",
    "                                              bias_initializer=zero_initializer, \n",
    "                                              activation=\"linear\",\n",
    "                                              name=\"element_\"+elements[i]+\"_atom_energy\")(hidden_layer[i][len(neuron_number)-1])\n",
    "\n",
    "concat_list=[]\n",
    "input_list=[]\n",
    "for i in range(len(elements)):\n",
    "    concat_list.append(atom_energy_layer[i])\n",
    "    input_list.append(input[i])\n",
    "    \n",
    "if len(elements)>1:\n",
    "    concat = keras.layers.concatenate(concat_list,axis=1,name=\"concat_atom_energy\") \n",
    "    output = keras.layers.Lambda(lambda x: K.sum(x, axis=1))(concat)\n",
    "else:\n",
    "    output = keras.layers.Lambda(lambda x: K.sum(x, axis=1))(concat_list[0])    \n",
    "\n",
    "model = keras.Model(inputs=input_list, outputs=output)\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "#opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss=\"mae\", optimizer=opt)\n",
    "\n",
    "model.summary()\n",
    "model.predict(x)\n",
    "\n",
    "#history = model.fit(x_train_list, y_train, epochs=5,batch_size=1, validation_data=(x_valid_list, y_valid))\n",
    "\n",
    "history = model.fit(x, y, epochs=5,batch_size=30)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
