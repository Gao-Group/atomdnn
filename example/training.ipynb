{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atomdnn\n",
    "\n",
    "# 'float32' is used for reading data and train by default, one can set data_type to 'float64' here\n",
    "atomdnn.data_type = 'float64'\n",
    "\n",
    "# force and stress are evaluated by default, \n",
    "# if one only need to compute potential energy, then set compute_force to false\n",
    "atomdnn.compute_force = True\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from atomdnn import data\n",
    "from atomdnn import network\n",
    "from atomdnn.data import Data\n",
    "from atomdnn.data import *\n",
    "from atomdnn.network import Network\n",
    "# import importlib\n",
    "# importlib.reload(atomdnn.data)\n",
    "# importlib.reload(atomdnn.network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data class from saved pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grdata = pickle.load(open(\"/mnt/machine_learning/grdata.pickle\", \"rb\", -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffel and then split the data into training, validation and test sets\n",
    "\n",
    "### split(self, train_data_percent=None, val_data_percent=None, test_data_percent=None, data_size=None)\n",
    "\n",
    "- **train_pct**: percentage of data used for training\n",
    "\n",
    "- **val_pct**: percentage of data used for validation\n",
    "\n",
    "- **test_pct**: percentage of data used for test\n",
    "\n",
    "- **data_size**: if not set, use the whole data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grdata.shuffel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning data: 407 images\n",
      "Validation data: 116 images\n",
      "Test data: 59 images\n"
     ]
    }
   ],
   "source": [
    "(x_train,y_train),(x_val,y_val),(x_test,y_test) = grdata.split(0.7,0.2,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Network object from class Network \n",
    "\n",
    "__init__(self, elements=None, num_fingerprints=None, arch=None,activation_function=None, data_type=None, import_dir=None)\n",
    "\n",
    "- **elements:** element list, required\n",
    "\n",
    "- **num_fingerprints:** number of fingerprints in data, required\n",
    "\n",
    "- **std**: = [mean, standard_deviation] of fingerprints, if set, standarlize the fingprints\n",
    "\n",
    "- **norm**: = [min, max] of fingerprints, if set, normalize the fingerprints\n",
    "\n",
    "- **arch:** number of layers of neural network\n",
    "\n",
    "- **activation_function:** if not set, default is 'tanh'\n",
    "\n",
    "- **import_dir:** read from the directory of a saved (imported) network, if used, all other parameters are disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation function is set to tanh by default.\n"
     ]
    }
   ],
   "source": [
    "model = Network(elements=['C'],num_fingerprints=grdata.num_fingerprints, std = [grdata.mean_fp,grdata.dev_fp],\n",
    "               arch=[50,50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "**train(self, train_input_dict, train_output_dict,\n",
    "              batch_size=None, epochs=None, loss_fn=None, optimizer=None, lr=None, train_force=False, train_stress=False)**\n",
    "\n",
    "- **train_input_dict**: input dictionary generated from build_dataset() for training\n",
    "    \n",
    "- **train_output_dict**: output dictionary generated from build_dataset() for training\n",
    "    \n",
    "- **batch_size**: if not set, use 30\n",
    "    \n",
    "- **epochs**: if not set, use 1\n",
    "    \n",
    "- **opimizer**: if not set, use Adam\n",
    "    \n",
    "- **lr**: learning rate, if not set, use 0.01\n",
    "    \n",
    "- **train_force**: if force used for training\n",
    "    \n",
    "- **train_stress**: if stress used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forces are not used for training.\n",
      "Stresses are not used for training.\n",
      "\n",
      "===> Epoch 1/500 - 1.308s/epoch\n",
      "     training_loss    - pe: 1.558\n",
      "     validation_loss  - pe: 3.447\n",
      "\n",
      "===> Epoch 2/500 - 1.310s/epoch\n",
      "     training_loss    - pe: 1.527\n",
      "     validation_loss  - pe: 2.860\n",
      "\n",
      "===> Epoch 3/500 - 1.276s/epoch\n",
      "     training_loss    - pe: 1.289\n",
      "     validation_loss  - pe: 2.612\n",
      "\n",
      "===> Epoch 4/500 - 1.244s/epoch\n",
      "     training_loss    - pe: 1.523\n",
      "     validation_loss  - pe: 3.957\n",
      "\n",
      "===> Epoch 5/500 - 1.276s/epoch\n",
      "     training_loss    - pe: 1.551\n",
      "     validation_loss  - pe: 2.400\n",
      "\n",
      "===> Epoch 6/500 - 1.347s/epoch\n",
      "     training_loss    - pe: 1.473\n",
      "     validation_loss  - pe: 2.212\n",
      "\n",
      "===> Epoch 7/500 - 1.276s/epoch\n",
      "     training_loss    - pe: 1.462\n",
      "     validation_loss  - pe: 1.611\n",
      "\n",
      "===> Epoch 8/500 - 1.292s/epoch\n",
      "     training_loss    - pe: 2.508\n",
      "     validation_loss  - pe: 2.586\n",
      "\n",
      "===> Epoch 9/500 - 1.388s/epoch\n",
      "     training_loss    - pe: 1.422\n",
      "     validation_loss  - pe: 2.914\n",
      "\n",
      "===> Epoch 10/500 - 1.259s/epoch\n",
      "     training_loss    - pe: 1.812\n",
      "     validation_loss  - pe: 2.518\n",
      "\n",
      "===> Epoch 11/500 - 1.255s/epoch\n",
      "     training_loss    - pe: 2.008\n",
      "     validation_loss  - pe: 2.515\n",
      "\n",
      "===> Epoch 12/500 - 1.290s/epoch\n",
      "     training_loss    - pe: 1.944\n",
      "     validation_loss  - pe: 3.230\n",
      "\n",
      "===> Epoch 13/500 - 1.342s/epoch\n",
      "     training_loss    - pe: 2.226\n",
      "     validation_loss  - pe: 3.150\n",
      "\n",
      "===> Epoch 14/500 - 1.266s/epoch\n",
      "     training_loss    - pe: 1.800\n",
      "     validation_loss  - pe: 2.924\n",
      "\n",
      "===> Epoch 15/500 - 1.222s/epoch\n",
      "     training_loss    - pe: 3.095\n",
      "     validation_loss  - pe: 2.707\n",
      "\n",
      "===> Epoch 16/500 - 1.248s/epoch\n",
      "     training_loss    - pe: 3.224\n",
      "     validation_loss  - pe: 3.295\n",
      "\n",
      "===> Epoch 17/500 - 1.276s/epoch\n",
      "     training_loss    - pe: 1.628\n",
      "     validation_loss  - pe: 2.490\n",
      "\n",
      "===> Epoch 18/500 - 1.332s/epoch\n",
      "     training_loss    - pe: 1.337\n",
      "     validation_loss  - pe: 2.416\n",
      "\n",
      "===> Epoch 19/500 - 1.336s/epoch\n",
      "     training_loss    - pe: 2.208\n",
      "     validation_loss  - pe: 1.371\n",
      "\n",
      "===> Epoch 20/500 - 1.256s/epoch\n",
      "     training_loss    - pe: 3.139\n",
      "     validation_loss  - pe: 2.754\n",
      "\n",
      "===> Epoch 21/500 - 1.252s/epoch\n",
      "     training_loss    - pe: 2.204\n",
      "     validation_loss  - pe: 3.460\n",
      "\n",
      "===> Epoch 22/500 - 1.279s/epoch\n",
      "     training_loss    - pe: 2.226\n",
      "     validation_loss  - pe: 3.446\n",
      "\n",
      "===> Epoch 23/500 - 1.242s/epoch\n",
      "     training_loss    - pe: 3.070\n",
      "     validation_loss  - pe: 2.579\n",
      "\n",
      "===> Epoch 24/500 - 1.279s/epoch\n",
      "     training_loss    - pe: 1.584\n",
      "     validation_loss  - pe: 3.387\n",
      "\n",
      "===> Epoch 25/500 - 1.284s/epoch\n",
      "     training_loss    - pe: 1.967\n",
      "     validation_loss  - pe: 2.645\n",
      "\n",
      "===> Epoch 26/500 - 1.293s/epoch\n",
      "     training_loss    - pe: 1.819\n",
      "     validation_loss  - pe: 2.396\n",
      "\n",
      "===> Epoch 27/500 - 1.277s/epoch\n",
      "     training_loss    - pe: 1.354\n",
      "     validation_loss  - pe: 1.716\n",
      "\n",
      "===> Epoch 28/500 - 1.399s/epoch\n",
      "     training_loss    - pe: 1.245\n",
      "     validation_loss  - pe: 3.176\n",
      "\n",
      "===> Epoch 29/500 - 1.415s/epoch\n",
      "     training_loss    - pe: 1.404\n",
      "     validation_loss  - pe: 2.925\n",
      "\n",
      "===> Epoch 30/500 - 1.383s/epoch\n",
      "     training_loss    - pe: 1.415\n",
      "     validation_loss  - pe: 2.717\n",
      "\n",
      "===> Epoch 31/500 - 1.631s/epoch\n",
      "     training_loss    - pe: 1.568\n",
      "     validation_loss  - pe: 2.092\n",
      "\n",
      "===> Epoch 32/500 - 1.351s/epoch\n",
      "     training_loss    - pe: 1.521\n",
      "     validation_loss  - pe: 3.241\n",
      "\n",
      "===> Epoch 33/500 - 1.281s/epoch\n",
      "     training_loss    - pe: 1.569\n",
      "     validation_loss  - pe: 2.517\n",
      "\n",
      "===> Epoch 34/500 - 1.291s/epoch\n",
      "     training_loss    - pe: 1.800\n",
      "     validation_loss  - pe: 2.397\n",
      "\n",
      "===> Epoch 35/500 - 1.289s/epoch\n",
      "     training_loss    - pe: 1.824\n",
      "     validation_loss  - pe: 2.081\n",
      "\n",
      "===> Epoch 36/500 - 1.296s/epoch\n",
      "     training_loss    - pe: 1.540\n",
      "     validation_loss  - pe: 2.719\n",
      "\n",
      "===> Epoch 37/500 - 1.274s/epoch\n",
      "     training_loss    - pe: 3.015\n",
      "     validation_loss  - pe: 3.468\n",
      "\n",
      "===> Epoch 38/500 - 1.279s/epoch\n",
      "     training_loss    - pe: 2.976\n",
      "     validation_loss  - pe: 3.399\n",
      "\n",
      "===> Epoch 39/500 - 1.275s/epoch\n",
      "     training_loss    - pe: 1.209\n",
      "     validation_loss  - pe: 3.031\n",
      "\n",
      "===> Epoch 40/500 - 1.279s/epoch\n",
      "     training_loss    - pe: 2.887\n",
      "     validation_loss  - pe: 4.090\n",
      "\n",
      "===> Epoch 41/500 - 1.290s/epoch\n",
      "     training_loss    - pe: 3.424\n",
      "     validation_loss  - pe: 2.396\n",
      "\n",
      "===> Epoch 42/500 - 1.317s/epoch\n",
      "     training_loss    - pe: 2.062\n",
      "     validation_loss  - pe: 2.021\n",
      "\n",
      "===> Epoch 43/500 - 1.315s/epoch\n",
      "     training_loss    - pe: 3.671\n",
      "     validation_loss  - pe: 1.610\n"
     ]
    }
   ],
   "source": [
    "model.train(x_train,y_train, validation_data=[x_val,y_val], batch_size=30, epochs=500,train_force=False,pe_loss_weight=0.01, force_loss_weight=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grdata.input_dict['dGdr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grdata.input_dict['fingerprints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.matmul(1/grdata.dev_fp*grdata.input_dict['dGdr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        pe_loss:       1.9471e+00\n",
      "     force_loss:       2.1933e+01\n",
      "    stress_loss:       3.6029e+05\n",
      "     total_loss:       1.9471e+00\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_val,y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction: compute potential energy, force and stress\n",
    "\n",
    "**predict (self, input_dict, training=False,compute_force=True)**\n",
    "\n",
    "- **input_dict**: input dictionary generated from build_dataset function\n",
    "    \n",
    "- **training**: set to False\n",
    "    \n",
    "- **compute_force**: if compute force, derivative data are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress_predict = tf.convert_to_tensor(stress_predict)\n",
    "mask = [True,True,True,False,True,True,False,False,True]\n",
    "tf.reshape(tf.boolean_mask(stress_predict, mask,axis=1),[-1,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.get('mae')\n",
    "\n",
    "pe_predict = model.predict(x_test)['pe']\n",
    "force_predict = model.predict(x_test)['force']\n",
    "stress_predict = model.predict(x_test)['stress']\n",
    "\n",
    "print(loss_fn(pe_predict,y_test['pe']))\n",
    "print(tf.reduce_mean(loss_fn(force_predict,y_test['force'])))\n",
    "print(tf.reduce_mean(loss_fn(stress_predict,y_test['stress'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__call__(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save trained model\n",
    "\n",
    "**save(obj, model_dir, descriptor=None)**\n",
    "\n",
    "- **obj**: Network object\n",
    "\n",
    "- **model_dir**: directory for saving the trained model\n",
    "\n",
    "- **descriptor**: descriptor parameters used to generate fingerprints, if set, a parameters file is generated for LAMMPS simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor = {'name': 'acsf', \n",
    "              'cutoff': 6.5001,\n",
    "              'etaG2':[0.01,0.025,0.05,0.075,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.8,1,1.5,2,3,5,10], \n",
    "              'etaG4': [0.01], \n",
    "              'zeta': [0.08,0.1,0.15,0.2,0.3,0.35,0.5,0.6,0.8,1.,1.5,2.,3.0,4.,5.5,7.0,10.0,25.0,50.0,100.0],\n",
    "              'lambda': [1.0, -1.0]}\n",
    "\n",
    "save_dir = 'graphene_24atoms.tfdnn'\n",
    "network.save(model, save_dir,descriptor=descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the saved model for continuous training and prediction\n",
    "\n",
    "**load(model_dir)**\n",
    "\n",
    "- **model_dir**: saved model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'graphene_24atoms.tfdnn'\n",
    "model = network.load(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print signature\n",
    "\n",
    "network.print_signature(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedata = data.read_inputdata_from_lmp(batch_mode=False, fp_filename='data_graphene_96atoms/dump_fingerprints.200',der_filename='data_graphene_96atoms/dump_fingerprints_der.200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peratom potential energy\n",
    "\n",
    "new_model.__call__(onedata.input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute peratom stress \n",
    "\n",
    "centerid = onedata.input_dict['center_atom_id']\n",
    "\n",
    "center_one_hot = tf.one_hot(centerid,depth=onedata.num_blocks,axis=1,dtype=onedata.data_type)\n",
    "\n",
    "stress_block = new_model.__call__(onedata.input_dict)['stress']\n",
    "\n",
    "stress_peratom = tf.matmul(center_one_hot,stress_block)\n",
    "\n",
    "evA2bar = 1602176\n",
    "\n",
    "for i in range(0,onedata.num_atoms):\n",
    "    print(i+1,\"          \",stress_peratom[0][i].numpy()*ev2bar)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_pe = new_model.__call__(onedata.input_dict)['atom_pe'][0]\n",
    "\n",
    "for i in range(0,onedata.num_atoms):\n",
    "    print(\"%d:   %.6g %.6 %.6 %.6 %.6 %.6 %.6 %.6\" % (i+1, atom_pe[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress = new_model.__call__(onedata.input_dict)['stress'][0]\n",
    "\n",
    "for i in range(0,onedata.num_atoms):\n",
    "    print(\"%d:   %.6g\" % (i+1, atom_pe[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.predict(onedata.input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "force = new_model.predict(onedata.input_dict)['force'][0]\n",
    "\n",
    "print (\"%s %5s %15s %15s\"%(\"atom_id\",\"f_x\",\"f_y\",\"f_z\"))\n",
    "for i in range(0,onedata.num_atoms):\n",
    "    print(\"%d %15.6f %15.6f %15.6f\" % (i+1,force[i][0].numpy(), force[i][1].numpy(), force[i][2].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue the training \n",
    "\n",
    "new_model.train(grdata.train_input_dict, grdata.train_output_dict, batch_size=30, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.predict(grdata.test_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check C_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose the second image to test\n",
    "image = data.slice_dict (grdata.input_dict,0,1)\n",
    "model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image['neighbor_atom_id'][0][23])\n",
    "print(image['neighbor_atom_coord'][0][23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__call__(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../c_inference/inference_energy \"../example/graphene_energy.tfdnn/\" \"../example/data_graphene_96atoms/dump_fingerprints.200\" 96  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdataset = tf.data.Dataset.from_tensor_slices((grdata.input_dict,grdata.output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(tfdataset, 'graphene_tfdataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdataset = tf.data.experimental.load('graphene_tfdataset',element_spec=tfdataset.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debuging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_model = network.load('graphene_energy.tfdnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe_model.predict(onedata.input_dict,compute_force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedata = data.read_inputdata_from_lmp(batch_mode=False, fp_filename='data_graphene_96atoms/dump_fingerprints.200',der_filename='data_graphene_96atoms/dump_fingerprints_der.200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(float('2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(onedata.input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(96):\n",
    "    print(i+1, model.predict(onedata.input_dict)['force'][0][i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onedata.input_dict['fingerprints'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%.8e'%onedata.input_dict['fingerprints'][0][1][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
