{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import random "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "element=['C']\n",
    "\n",
    "image_num=300\n",
    "\n",
    "atom_num=[24]\n",
    "\n",
    "fingerprint_num=59\n",
    "\n",
    "# read fingerprints\n",
    "filepath='/workspace/data/group_share/data_graphene_ase/deform/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read fingerprints and potential energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints={}  # define fingerprints as a dictionary to store fingerprints descriptors, each entry corresponds to one type of element\n",
    "cont={} # define cont as a dictionary to index the atom for each element\n",
    "for i in range(len(element)): # initialize fingerprints and cont\n",
    "    fingerprints[i]=np.zeros((image_num,atom_num[i],fingerprint_num),dtype='float64') # each element in fingerprints is a 3 dimentional matrix\n",
    "    cont[i]=0\n",
    "\n",
    "\n",
    "n=0 # image counter\n",
    "for filenum in range(1,image_num+1):\n",
    "    for k in range(len(cont)): # cont index the atom\n",
    "        cont[k]=0\n",
    "    file = open(filepath+'/dump_fp.'+str(filenum))\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    str_data = np.array([lines[i].split() for i in range(9,len(lines))])\n",
    "    for i in range(sum(atom_num)):\n",
    "        j = str_data[i][1].astype(int) - 1 # get atom element type, minus 1 so that index starting from 0\n",
    "        fingerprints[j][n][cont[j]] = str_data[i][2:].astype('float64')\n",
    "        cont[j]=cont[j]+1\n",
    "    n=n+1\n",
    "\n",
    "# read potential energy\n",
    "def read_outputdata(pe, filename=None):\n",
    "    try:\n",
    "        file = open(filename)\n",
    "    except OSError:\n",
    "        raise OSError('Could not open file %s.' % filename)\n",
    "\n",
    "    lines = file.readlines()    \n",
    "    file.close()\n",
    "\n",
    "    fd = 0 # count image number\n",
    "    check_pe = 0\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        if 'potential_energy' in lines[i]:\n",
    "            check_pe = 1\n",
    "        elif check_pe == 1:\n",
    "            pe.append(np.float64(lines[i]))\n",
    "            check_pe = 0\n",
    "\n",
    "pe = []\n",
    "outfile = '/workspace/data/group_share/data_graphene_ase/deform/lmp_output.dat'\n",
    "read_outputdata(pe, filename=outfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize/Min-max scaling data, fingerprints range from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxfp = tf.math.reduce_max(fingerprints[0],axis=[0,1])\n",
    "# minfp = tf.math.reduce_min(fingerprints[0],axis=[0,1])\n",
    "# fingerprints[0] = (fingerprints[0] - minfp)/(maxfp - minfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack = fingerprints[0]\n",
    "# for i in range(1,len(element)):\n",
    "#     stack = np.vstack((stack,fingerprints[i]))\n",
    "# min_fingerprint = stack.min()\n",
    "# max_fingerprint = stack.max()\n",
    "# del stack\n",
    "\n",
    "# print ('minmum fingerprints = ',min_fingerprint)\n",
    "# print ('maximum fingerprints = ',max_fingerprint)\n",
    "\n",
    "# for i in range(len(element)):\n",
    "#     fingerprints[i] = fingerprints[i]/(max_fingerprint - min_fingerprint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle the fingerprints and potential energy simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped = list(zip(*[fingerprints[i] for i in range(len(element))], pe))\n",
    "random.shuffle(zipped) \n",
    "unzipped= list(zip(*zipped)) \n",
    "for i in range(len(element)):\n",
    "    fingerprints[i] = unzipped[i]\n",
    "pe = np.array(unzipped[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break the data into training, validation and test sets for Keras tranning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= 210\n",
    "valid_data= 60\n",
    "test_data=30\n",
    "\n",
    "data_partition=[train_data,train_data+valid_data]\n",
    "\n",
    "x_train, x_valid, x_test = np.array_split(fingerprints[0],data_partition)\n",
    "#X_train_B, X_valid_B, X_test_B, X_check_B = np.array_split(fingerprints[1],data_partition)\n",
    "\n",
    "y_train, y_valid, y_test = np.array_split(pe,data_partition)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 03:20:45.549396: W tensorflow/stream_executor/platform/default/dso_loader.cc:65] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-10-09 03:20:45.549482: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-10-09 03:20:45.549879: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n"
     ]
    }
   ],
   "source": [
    "maxfp = tf.math.reduce_max(x_train,axis=[0,1])\n",
    "minfp = tf.math.reduce_min(x_train,axis=[0,1])\n",
    "x_train = (x_train - minfp)/(maxfp - minfp)\n",
    "x_valid = (x_valid - minfp)/(maxfp - minfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create tensorflow dataset for training without Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tf.Tensor(\n",
      "[[[1.         1.         1.         ... 1.         1.         1.        ]\n",
      "  [1.         1.         1.         ... 1.         1.         1.        ]\n",
      "  [1.         1.         1.         ... 1.         1.         1.        ]\n",
      "  ...\n",
      "  [1.         1.         1.         ... 1.         1.         1.        ]\n",
      "  [1.         1.         1.         ... 1.         1.         1.        ]\n",
      "  [1.         1.         1.         ... 1.         1.         1.        ]]\n",
      "\n",
      " [[0.94804955 0.94819586 0.94832316 ... 0.93121084 0.93014294 0.93223607]\n",
      "  [0.94804955 0.94819586 0.94832316 ... 0.93121084 0.93014294 0.93223607]\n",
      "  [0.94804955 0.94819586 0.94832316 ... 0.93121084 0.93014294 0.93223607]\n",
      "  ...\n",
      "  [0.94804955 0.94819586 0.94832316 ... 0.93121084 0.93014294 0.93223607]\n",
      "  [0.94804955 0.94819586 0.94832316 ... 0.93121084 0.93014294 0.93223607]\n",
      "  [0.94804955 0.94819586 0.94832316 ... 0.93121084 0.93014294 0.93223607]]\n",
      "\n",
      " [[0.897998   0.89832246 0.89860379 ... 0.8666614  0.86448894 0.86619168]\n",
      "  [0.897998   0.89832246 0.89860379 ... 0.8666614  0.86448894 0.86619168]\n",
      "  [0.897998   0.89832246 0.89860379 ... 0.8666614  0.86448894 0.86619168]\n",
      "  ...\n",
      "  [0.897998   0.89832246 0.89860379 ... 0.8666614  0.86448894 0.86619168]\n",
      "  [0.897998   0.89832246 0.89860379 ... 0.8666614  0.86448894 0.86619168]\n",
      "  [0.897998   0.89832246 0.89860379 ... 0.8666614  0.86448894 0.86619168]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.44996151 0.44994077 0.4497566  ... 0.3865998  0.36255932 0.36901838]\n",
      "  [0.44996151 0.44994077 0.4497566  ... 0.3865998  0.36255932 0.36901838]\n",
      "  [0.44996151 0.44994077 0.4497566  ... 0.3865998  0.36255932 0.36901838]\n",
      "  ...\n",
      "  [0.44996151 0.44994077 0.4497566  ... 0.3865998  0.36255932 0.36901838]\n",
      "  [0.44996151 0.44994077 0.4497566  ... 0.3865998  0.36255932 0.36901838]\n",
      "  [0.44996151 0.44994077 0.4497566  ... 0.3865998  0.36255932 0.36901838]]\n",
      "\n",
      " [[0.4162886  0.4164293  0.41642434 ... 0.35964019 0.3327171  0.34248933]\n",
      "  [0.4162886  0.4164293  0.41642434 ... 0.35964019 0.3327171  0.34248933]\n",
      "  [0.4162886  0.4164293  0.41642434 ... 0.35964019 0.3327171  0.34248933]\n",
      "  ...\n",
      "  [0.4162886  0.4164293  0.41642434 ... 0.35964019 0.3327171  0.34248933]\n",
      "  [0.4162886  0.4164293  0.41642434 ... 0.35964019 0.3327171  0.34248933]\n",
      "  [0.4162886  0.4164293  0.41642434 ... 0.35964019 0.3327171  0.34248933]]\n",
      "\n",
      " [[0.38364367 0.38396123 0.38414547 ... 0.33458982 0.30494216 0.3183524 ]\n",
      "  [0.38364367 0.38396123 0.38414547 ... 0.33458982 0.30494216 0.3183524 ]\n",
      "  [0.38364367 0.38396123 0.38414547 ... 0.33458982 0.30494216 0.3183524 ]\n",
      "  ...\n",
      "  [0.38364367 0.38396123 0.38414547 ... 0.33458982 0.30494216 0.3183524 ]\n",
      "  [0.38364367 0.38396123 0.38414547 ... 0.33458982 0.30494216 0.3183524 ]\n",
      "  [0.38364367 0.38396123 0.38414547 ... 0.33458982 0.30494216 0.3183524 ]]], shape=(30, 24, 59), dtype=float64) tf.Tensor(\n",
      "[-161.799678 -165.368799 -168.190678 -170.313326 -171.792795 -172.688284\n",
      " -173.05899  -172.962127 -172.451757 -171.578164 -170.387592 -168.922233\n",
      " -167.220354 -165.316509 -163.241794 -165.345077 -168.509759 -170.969788\n",
      " -172.792641 -174.021508 -174.705914 -174.897722 -174.648551 -174.008169\n",
      " -173.023541 -171.738327 -170.192668 -168.423178 -166.463043 -164.342205], shape=(30,), dtype=float64)\n",
      "1 tf.Tensor(\n",
      "[[[0.89800552 0.89832842 0.89860797 ... 0.86710355 0.86471917 0.8680626 ]\n",
      "  [0.89800552 0.89832842 0.89860797 ... 0.86710355 0.86471917 0.8680626 ]\n",
      "  [0.89800552 0.89832842 0.89860797 ... 0.86710355 0.86471917 0.8680626 ]\n",
      "  ...\n",
      "  [0.89800552 0.89832842 0.89860797 ... 0.86710355 0.86471917 0.8680626 ]\n",
      "  [0.89800552 0.89832842 0.89860797 ... 0.86710355 0.86471917 0.8680626 ]\n",
      "  [0.89800552 0.89832842 0.89860797 ... 0.86710355 0.86471917 0.8680626 ]]\n",
      "\n",
      " [[0.84803505 0.84851639 0.84892852 ... 0.80399548 0.80172731 0.80894793]\n",
      "  [0.84803505 0.84851639 0.84892852 ... 0.80399548 0.80172731 0.80894793]\n",
      "  [0.84803505 0.84851639 0.84892852 ... 0.80399548 0.80172731 0.80894793]\n",
      "  ...\n",
      "  [0.84803505 0.84851639 0.84892852 ... 0.80399548 0.80172731 0.80894793]\n",
      "  [0.84803505 0.84851639 0.84892852 ... 0.80399548 0.80172731 0.80894793]\n",
      "  [0.84803505 0.84851639 0.84892852 ... 0.80399548 0.80172731 0.80894793]]\n",
      "\n",
      " [[0.79997048 0.80061842 0.80116555 ... 0.74497533 0.74266283 0.75196542]\n",
      "  [0.79997048 0.80061842 0.80116555 ... 0.74497533 0.74266283 0.75196542]\n",
      "  [0.79997048 0.80061842 0.80116555 ... 0.74497533 0.74266283 0.75196542]\n",
      "  ...\n",
      "  [0.79997048 0.80061842 0.80116555 ... 0.74497533 0.74266283 0.75196542]\n",
      "  [0.79997048 0.80061842 0.80116555 ... 0.74497533 0.74266283 0.75196542]\n",
      "  [0.79997048 0.80061842 0.80116555 ... 0.74497533 0.74266283 0.75196542]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.37168329 0.37169695 0.37148549 ... 0.30262963 0.28772482 0.29540562]\n",
      "  [0.37168329 0.37169695 0.37148549 ... 0.30262963 0.28772482 0.29540562]\n",
      "  [0.37168329 0.37169695 0.37148549 ... 0.30262963 0.28772482 0.29540562]\n",
      "  ...\n",
      "  [0.37168329 0.37169695 0.37148549 ... 0.30262963 0.28772482 0.29540562]\n",
      "  [0.37168329 0.37169695 0.37148549 ... 0.30262963 0.28772482 0.29540562]\n",
      "  [0.37168329 0.37169695 0.37148549 ... 0.30262963 0.28772482 0.29540562]]\n",
      "\n",
      " [[0.33934364 0.33949532 0.33943969 ... 0.27768335 0.26030067 0.27030139]\n",
      "  [0.33934364 0.33949532 0.33943969 ... 0.27768335 0.26030067 0.27030139]\n",
      "  [0.33934364 0.33949532 0.33943969 ... 0.27768335 0.26030067 0.27030139]\n",
      "  ...\n",
      "  [0.33934364 0.33949532 0.33943969 ... 0.27768335 0.26030067 0.27030139]\n",
      "  [0.33934364 0.33949532 0.33943969 ... 0.27768335 0.26030067 0.27030139]\n",
      "  [0.33934364 0.33949532 0.33943969 ... 0.27768335 0.26030067 0.27030139]]\n",
      "\n",
      " [[0.30803364 0.30832654 0.30842481 ... 0.25456752 0.23476413 0.24742556]\n",
      "  [0.30803364 0.30832654 0.30842481 ... 0.25456752 0.23476413 0.24742556]\n",
      "  [0.30803364 0.30832654 0.30842481 ... 0.25456752 0.23476413 0.24742556]\n",
      "  ...\n",
      "  [0.30803364 0.30832654 0.30842481 ... 0.25456752 0.23476413 0.24742556]\n",
      "  [0.30803364 0.30832654 0.30842481 ... 0.25456752 0.23476413 0.24742556]\n",
      "  [0.30803364 0.30832654 0.30842481 ... 0.25456752 0.23476413 0.24742556]]], shape=(30, 24, 59), dtype=float64) tf.Tensor(\n",
      "[-168.050241 -170.950398 -173.091681 -174.631076 -175.6276   -176.12023\n",
      " -176.152889 -175.771162 -175.020198 -173.943397 -172.581643 -170.972895\n",
      " -169.152024 -167.150804 -164.998003 -169.949949 -172.677677 -174.615288\n",
      " -175.906818 -176.684616 -177.000457 -176.889642 -176.391275 -175.545573\n",
      " -174.392151 -172.968959 -171.311659 -169.453304 -167.424207 -165.251941], shape=(30,), dtype=float64)\n",
      "2 tf.Tensor(\n",
      "[[[0.80397604 0.80451613 0.8049467  ... 0.75223978 0.74647015 0.75046767]\n",
      "  [0.80397604 0.80451613 0.8049467  ... 0.75223978 0.74647015 0.75046767]\n",
      "  [0.80397604 0.80451613 0.8049467  ... 0.75223978 0.74647015 0.75046767]\n",
      "  ...\n",
      "  [0.80397604 0.80451613 0.8049467  ... 0.75223978 0.74647015 0.75046767]\n",
      "  [0.80397604 0.80451613 0.8049467  ... 0.75223978 0.74647015 0.75046767]\n",
      "  [0.80397604 0.80451613 0.8049467  ... 0.75223978 0.74647015 0.75046767]]\n",
      "\n",
      " [[0.75597123 0.75664135 0.75716942 ... 0.69338382 0.6891467  0.69620443]\n",
      "  [0.75597123 0.75664135 0.75716942 ... 0.69338382 0.6891467  0.69620443]\n",
      "  [0.75597123 0.75664135 0.75716942 ... 0.69338382 0.6891467  0.69620443]\n",
      "  ...\n",
      "  [0.75597123 0.75664135 0.75716942 ... 0.69338382 0.6891467  0.69620443]\n",
      "  [0.75597123 0.75664135 0.75716942 ... 0.69338382 0.6891467  0.69620443]\n",
      "  [0.75597123 0.75664135 0.75716942 ... 0.69338382 0.6891467  0.69620443]]\n",
      "\n",
      " [[0.70987815 0.71066252 0.71126717 ... 0.63849721 0.63552975 0.64436258]\n",
      "  [0.70987815 0.71066252 0.71126717 ... 0.63849721 0.63552975 0.64436258]\n",
      "  [0.70987815 0.71066252 0.71126717 ... 0.63849721 0.63552975 0.64436258]\n",
      "  ...\n",
      "  [0.70987815 0.71066252 0.71126717 ... 0.63849721 0.63552975 0.64436258]\n",
      "  [0.70987815 0.71066252 0.71126717 ... 0.63849721 0.63552975 0.64436258]\n",
      "  [0.70987815 0.71066252 0.71126717 ... 0.63849721 0.63552975 0.64436258]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.30062231 0.30017603 0.29946272 ... 0.23107506 0.22350184 0.22932542]\n",
      "  [0.30062231 0.30017603 0.29946272 ... 0.23107506 0.22350184 0.22932542]\n",
      "  [0.30062231 0.30017603 0.29946272 ... 0.23107506 0.22350184 0.22932542]\n",
      "  ...\n",
      "  [0.30062231 0.30017603 0.29946272 ... 0.23107506 0.22350184 0.22932542]\n",
      "  [0.30062231 0.30017603 0.29946272 ... 0.23107506 0.22350184 0.22932542]\n",
      "  [0.30062231 0.30017603 0.29946272 ... 0.23107506 0.22350184 0.22932542]]\n",
      "\n",
      " [[0.26956044 0.26921207 0.26861807 ... 0.20795259 0.19839108 0.2058486 ]\n",
      "  [0.26956044 0.26921207 0.26861807 ... 0.20795259 0.19839108 0.2058486 ]\n",
      "  [0.26956044 0.26921207 0.26861807 ... 0.20795259 0.19839108 0.2058486 ]\n",
      "  ...\n",
      "  [0.26956044 0.26921207 0.26861807 ... 0.20795259 0.19839108 0.2058486 ]\n",
      "  [0.26956044 0.26921207 0.26861807 ... 0.20795259 0.19839108 0.2058486 ]\n",
      "  [0.26956044 0.26921207 0.26861807 ... 0.20795259 0.19839108 0.2058486 ]]\n",
      "\n",
      " [[0.23946816 0.23922638 0.23875612 ... 0.18654658 0.1749719  0.18438883]\n",
      "  [0.23946816 0.23922638 0.23875612 ... 0.18654658 0.1749719  0.18438883]\n",
      "  [0.23946816 0.23922638 0.23875612 ... 0.18654658 0.1749719  0.18438883]\n",
      "  ...\n",
      "  [0.23946816 0.23922638 0.23875612 ... 0.18654658 0.1749719  0.18438883]\n",
      "  [0.23946816 0.23922638 0.23875612 ... 0.18654658 0.1749719  0.18438883]\n",
      "  [0.23946816 0.23922638 0.23875612 ... 0.18654658 0.1749719  0.18438883]]], shape=(30, 24, 59), dtype=float64) tf.Tensor(\n",
      "[-171.116299 -173.723901 -175.533941 -176.67183  -177.260208 -177.410593\n",
      " -177.168555 -176.566193 -175.638469 -174.421002 -172.94867  -171.254751\n",
      " -169.37042  -167.324497 -165.143345 -171.63138  -174.152155 -175.877948\n",
      " -176.924662 -177.400314 -177.409272 -177.045417 -176.349011 -175.349371\n",
      " -174.077894 -172.56626  -170.845286 -168.94424  -166.890432 -164.709018], shape=(30,), dtype=float64)\n",
      "3 tf.Tensor(\n",
      "[[[0.71778606 0.7181842  0.71841223 ... 0.65411184 0.64364823 0.64923999]\n",
      "  [0.71778606 0.7181842  0.71841223 ... 0.65411184 0.64364823 0.64923999]\n",
      "  [0.71778606 0.7181842  0.71841223 ... 0.65411184 0.64364823 0.64923999]\n",
      "  ...\n",
      "  [0.71778606 0.7181842  0.71841223 ... 0.65411184 0.64364823 0.64923999]\n",
      "  [0.71778606 0.7181842  0.71841223 ... 0.65411184 0.64364823 0.64923999]\n",
      "  [0.71778606 0.7181842  0.71841223 ... 0.65411184 0.64364823 0.64923999]]\n",
      "\n",
      " [[0.67170173 0.67217052 0.67243253 ... 0.59889652 0.59121429 0.59869792]\n",
      "  [0.67170173 0.67217052 0.67243253 ... 0.59889652 0.59121429 0.59869792]\n",
      "  [0.67170173 0.67217052 0.67243253 ... 0.59889652 0.59121429 0.59869792]\n",
      "  ...\n",
      "  [0.67170173 0.67217052 0.67243253 ... 0.59889652 0.59121429 0.59869792]\n",
      "  [0.67170173 0.67217052 0.67243253 ... 0.59889652 0.59121429 0.59869792]\n",
      "  [0.67170173 0.67217052 0.67243253 ... 0.59889652 0.59121429 0.59869792]]\n",
      "\n",
      " [[0.62752771 0.62802901 0.62828359 ... 0.54740366 0.54221362 0.55038085]\n",
      "  [0.62752771 0.62802901 0.62828359 ... 0.54740366 0.54221362 0.55038085]\n",
      "  [0.62752771 0.62802901 0.62828359 ... 0.54740366 0.54221362 0.55038085]\n",
      "  ...\n",
      "  [0.62752771 0.62802901 0.62828359 ... 0.54740366 0.54221362 0.55038085]\n",
      "  [0.62752771 0.62802901 0.62828359 ... 0.54740366 0.54221362 0.55038085]\n",
      "  [0.62752771 0.62802901 0.62828359 ... 0.54740366 0.54221362 0.55038085]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.23417435 0.23329205 0.23216381 ... 0.16988134 0.16724007 0.17038957]\n",
      "  [0.23417435 0.23329205 0.23216381 ... 0.16988134 0.16724007 0.17038957]\n",
      "  [0.23417435 0.23329205 0.23216381 ... 0.16988134 0.16724007 0.17038957]\n",
      "  ...\n",
      "  [0.23417435 0.23329205 0.23216381 ... 0.16988134 0.16724007 0.17038957]\n",
      "  [0.23417435 0.23329205 0.23216381 ... 0.16988134 0.16724007 0.17038957]\n",
      "  [0.23417435 0.23329205 0.23216381 ... 0.16988134 0.16724007 0.17038957]]\n",
      "\n",
      " [[0.20407308 0.2033147  0.20233736 ... 0.14827083 0.14415806 0.14845028]\n",
      "  [0.20407308 0.2033147  0.20233736 ... 0.14827083 0.14415806 0.14845028]\n",
      "  [0.20407308 0.2033147  0.20233736 ... 0.14827083 0.14415806 0.14845028]\n",
      "  ...\n",
      "  [0.20407308 0.2033147  0.20233736 ... 0.14827083 0.14415806 0.14845028]\n",
      "  [0.20407308 0.2033147  0.20233736 ... 0.14827083 0.14415806 0.14845028]\n",
      "  [0.20407308 0.2033147  0.20233736 ... 0.14827083 0.14415806 0.14845028]]\n",
      "\n",
      " [[0.17492142 0.17429237 0.17346692 ... 0.12830402 0.12263383 0.12837918]\n",
      "  [0.17492142 0.17429237 0.17346692 ... 0.12830402 0.12263383 0.12837918]\n",
      "  [0.17492142 0.17429237 0.17346692 ... 0.12830402 0.12263383 0.12837918]\n",
      "  ...\n",
      "  [0.17492142 0.17429237 0.17346692 ... 0.12830402 0.12263383 0.12837918]\n",
      "  [0.17492142 0.17429237 0.17346692 ... 0.12830402 0.12263383 0.12837918]\n",
      "  [0.17492142 0.17429237 0.17346692 ... 0.12830402 0.12263383 0.12837918]]], shape=(30, 24, 59), dtype=float64) tf.Tensor(\n",
      "[-171.576272 -174.034085 -175.702994 -176.69405  -177.107787 -177.03724\n",
      " -176.571156 -175.788474 -174.724918 -173.407392 -171.864232 -170.123737\n",
      " -168.213241 -166.158557 -163.983656 -171.026842 -173.441016 -175.072099\n",
      " -176.029645 -176.410165 -176.300935 -175.782064 -174.928934 -173.807818\n",
      " -172.450571 -170.882015 -169.127856 -167.213492 -165.163262 -162.999993], shape=(30,), dtype=float64)\n",
      "4 tf.Tensor(\n",
      "[[[0.63732543 0.63760499 0.63768006 ... 0.57056115 0.55398346 0.56461259]\n",
      "  [0.63732543 0.63760499 0.63768006 ... 0.57056115 0.55398346 0.56461259]\n",
      "  [0.63732543 0.63760499 0.63768006 ... 0.57056115 0.55398346 0.56461259]\n",
      "  ...\n",
      "  [0.63732543 0.63760499 0.63768006 ... 0.57056115 0.55398346 0.56461259]\n",
      "  [0.63732543 0.63760499 0.63768006 ... 0.57056115 0.55398346 0.56461259]\n",
      "  [0.63732543 0.63760499 0.63768006 ... 0.57056115 0.55398346 0.56461259]]\n",
      "\n",
      " [[0.5931303  0.59338807 0.59340478 ... 0.51836096 0.50571586 0.516513  ]\n",
      "  [0.5931303  0.59338807 0.59340478 ... 0.51836096 0.50571586 0.516513  ]\n",
      "  [0.5931303  0.59338807 0.59340478 ... 0.51836096 0.50571586 0.516513  ]\n",
      "  ...\n",
      "  [0.5931303  0.59338807 0.59340478 ... 0.51836096 0.50571586 0.516513  ]\n",
      "  [0.5931303  0.59338807 0.59340478 ... 0.51836096 0.50571586 0.516513  ]\n",
      "  [0.5931303  0.59338807 0.59340478 ... 0.51836096 0.50571586 0.516513  ]]\n",
      "\n",
      " [[0.55084165 0.55101775 0.55091576 ... 0.46976802 0.46069371 0.47078002]\n",
      "  [0.55084165 0.55101775 0.55091576 ... 0.46976802 0.46069371 0.47078002]\n",
      "  [0.55084165 0.55101775 0.55091576 ... 0.46976802 0.46069371 0.47078002]\n",
      "  ...\n",
      "  [0.55084165 0.55101775 0.55091576 ... 0.46976802 0.46069371 0.47078002]\n",
      "  [0.55084165 0.55101775 0.55091576 ... 0.46976802 0.46069371 0.47078002]\n",
      "  [0.55084165 0.55101775 0.55091576 ... 0.46976802 0.46069371 0.47078002]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.17080897 0.16986936 0.16874791 ... 0.11722601 0.11729657 0.11822835]\n",
      "  [0.17080897 0.16986936 0.16874791 ... 0.11722601 0.11729657 0.11822835]\n",
      "  [0.17080897 0.16986936 0.16874791 ... 0.11722601 0.11729657 0.11822835]\n",
      "  ...\n",
      "  [0.17080897 0.16986936 0.16874791 ... 0.11722601 0.11729657 0.11822835]\n",
      "  [0.17080897 0.16986936 0.16874791 ... 0.11722601 0.11729657 0.11822835]\n",
      "  [0.17080897 0.16986936 0.16874791 ... 0.11722601 0.11729657 0.11822835]]\n",
      "\n",
      " [[0.14161078 0.14082367 0.13988428 ... 0.09700427 0.09610132 0.09775884]\n",
      "  [0.14161078 0.14082367 0.13988428 ... 0.09700427 0.09610132 0.09775884]\n",
      "  [0.14161078 0.14082367 0.13988428 ... 0.09700427 0.09610132 0.09775884]\n",
      "  ...\n",
      "  [0.14161078 0.14082367 0.13988428 ... 0.09700427 0.09610132 0.09775884]\n",
      "  [0.14161078 0.14082367 0.13988428 ... 0.09700427 0.09610132 0.09775884]\n",
      "  [0.14161078 0.14082367 0.13988428 ... 0.09700427 0.09610132 0.09775884]]\n",
      "\n",
      " [[0.11335307 0.11271784 0.11195535 ... 0.07835775 0.07635196 0.07901663]\n",
      "  [0.11335307 0.11271784 0.11195535 ... 0.07835775 0.07635196 0.07901663]\n",
      "  [0.11335307 0.11271784 0.11195535 ... 0.07835775 0.07635196 0.07901663]\n",
      "  ...\n",
      "  [0.11335307 0.11271784 0.11195535 ... 0.07835775 0.07635196 0.07901663]\n",
      "  [0.11335307 0.11271784 0.11195535 ... 0.07835775 0.07635196 0.07901663]\n",
      "  [0.11335307 0.11271784 0.11195535 ... 0.07835775 0.07635196 0.07901663]]], shape=(30, 24, 59), dtype=float64) tf.Tensor(\n",
      "[-170.053216 -172.440622 -174.048483 -174.987429 -175.352252 -175.227049\n",
      " -174.687619 -173.802903 -172.636842 -171.244983 -169.655898 -167.892618\n",
      " -165.978598 -163.936742 -161.788799 -168.721496 -171.096421 -172.692922\n",
      " -173.623995 -173.984372 -173.856565 -173.313995 -172.42249  -171.241342\n",
      " -169.824704 -168.219059 -166.450212 -164.539671 -162.508964 -160.378847], shape=(30,), dtype=float64)\n",
      "5 tf.Tensor(\n",
      "[[[0.56266539 0.56267176 0.56246255 ... 0.49948976 0.47621174 0.49481871]\n",
      "  [0.56266539 0.56267176 0.56246255 ... 0.49948976 0.47621174 0.49481871]\n",
      "  [0.56266539 0.56267176 0.56246255 ... 0.49948976 0.47621174 0.49481871]\n",
      "  ...\n",
      "  [0.56266539 0.56267176 0.56246255 ... 0.49948976 0.47621174 0.49481871]\n",
      "  [0.56266539 0.56267176 0.56246255 ... 0.49948976 0.47621174 0.49481871]\n",
      "  [0.56266539 0.56267176 0.56246255 ... 0.49948976 0.47621174 0.49481871]]\n",
      "\n",
      " [[0.52016234 0.52006854 0.51972971 ... 0.4498802  0.43152111 0.44867866]\n",
      "  [0.52016234 0.52006854 0.51972971 ... 0.4498802  0.43152111 0.44867866]\n",
      "  [0.52016234 0.52006854 0.51972971 ... 0.4498802  0.43152111 0.44867866]\n",
      "  ...\n",
      "  [0.52016234 0.52006854 0.51972971 ... 0.4498802  0.43152111 0.44867866]\n",
      "  [0.52016234 0.52006854 0.51972971 ... 0.4498802  0.43152111 0.44867866]\n",
      "  [0.52016234 0.52006854 0.51972971 ... 0.4498802  0.43152111 0.44867866]]\n",
      "\n",
      " [[0.47927463 0.47908491 0.47862281 ... 0.40357682 0.38967958 0.40451387]\n",
      "  [0.47927463 0.47908491 0.47862281 ... 0.40357682 0.38967958 0.40451387]\n",
      "  [0.47927463 0.47908491 0.47862281 ... 0.40357682 0.38967958 0.40451387]\n",
      "  ...\n",
      "  [0.47927463 0.47908491 0.47862281 ... 0.40357682 0.38967958 0.40451387]\n",
      "  [0.47927463 0.47908491 0.47862281 ... 0.40357682 0.38967958 0.40451387]\n",
      "  [0.47927463 0.47908491 0.47862281 ... 0.40357682 0.38967958 0.40451387]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.110693   0.109969   0.10913592 ... 0.07221697 0.07318763 0.07289777]\n",
      "  [0.110693   0.109969   0.10913592 ... 0.07221697 0.07318763 0.07289777]\n",
      "  [0.110693   0.109969   0.10913592 ... 0.07221697 0.07318763 0.07289777]\n",
      "  ...\n",
      "  [0.110693   0.109969   0.10913592 ... 0.07221697 0.07318763 0.07289777]\n",
      "  [0.110693   0.109969   0.10913592 ... 0.07221697 0.07318763 0.07289777]\n",
      "  [0.110693   0.109969   0.10913592 ... 0.07221697 0.07318763 0.07289777]]\n",
      "\n",
      " [[0.08240468 0.08184624 0.08120794 ... 0.05330431 0.05374156 0.05380218]\n",
      "  [0.08240468 0.08184624 0.08120794 ... 0.05330431 0.05374156 0.05380218]\n",
      "  [0.08240468 0.08184624 0.08120794 ... 0.05330431 0.05374156 0.05380218]\n",
      "  ...\n",
      "  [0.08240468 0.08184624 0.08120794 ... 0.05330431 0.05374156 0.05380218]\n",
      "  [0.08240468 0.08184624 0.08120794 ... 0.05330431 0.05374156 0.05380218]\n",
      "  [0.08240468 0.08184624 0.08120794 ... 0.05330431 0.05374156 0.05380218]]\n",
      "\n",
      " [[0.05496075 0.05458614 0.05415656 ... 0.03584957 0.03560867 0.03625522]\n",
      "  [0.05496075 0.05458614 0.05415656 ... 0.03584957 0.03560867 0.03625522]\n",
      "  [0.05496075 0.05458614 0.05415656 ... 0.03584957 0.03560867 0.03625522]\n",
      "  ...\n",
      "  [0.05496075 0.05458614 0.05415656 ... 0.03584957 0.03560867 0.03625522]\n",
      "  [0.05496075 0.05458614 0.05415656 ... 0.03584957 0.03560867 0.03625522]\n",
      "  [0.05496075 0.05458614 0.05415656 ... 0.03584957 0.03560867 0.03625522]]], shape=(30, 24, 59), dtype=float64) tf.Tensor(\n",
      "[-167.099235 -169.46859  -171.063271 -171.994445 -172.357811 -172.235362\n",
      " -171.699264 -170.813686 -169.635785 -168.216515 -166.601673 -164.83013\n",
      " -162.925393 -160.907689 -158.796901 -165.250265 -167.614963 -169.214864\n",
      " -170.151905 -170.523109 -170.410576 -169.885923 -169.012469 -167.846323\n",
      " -166.437093 -164.828547 -163.059398 -161.162202 -159.15861  -157.067794], shape=(30,), dtype=float64)\n",
      "6 tf.Tensor(\n",
      "[[[0.49260198 0.492368   0.49194049 ... 0.43748306 0.40809403 0.43549168]\n",
      "  [0.49260198 0.492368   0.49194049 ... 0.43748306 0.40809403 0.43549168]\n",
      "  [0.49260198 0.492368   0.49194049 ... 0.43748306 0.40809403 0.43549168]\n",
      "  ...\n",
      "  [0.49260198 0.492368   0.49194049 ... 0.43748306 0.40809403 0.43549168]\n",
      "  [0.49260198 0.492368   0.49194049 ... 0.43748306 0.40809403 0.43549168]\n",
      "  [0.49260198 0.492368   0.49194049 ... 0.43748306 0.40809403 0.43549168]]\n",
      "\n",
      " [[0.45125288 0.45096381 0.45046236 ... 0.39000714 0.36660215 0.39120374]\n",
      "  [0.45125288 0.45096381 0.45046236 ... 0.39000714 0.36660215 0.39120374]\n",
      "  [0.45125288 0.45096381 0.45046236 ... 0.39000714 0.36660215 0.39120374]\n",
      "  ...\n",
      "  [0.45125288 0.45096381 0.45046236 ... 0.39000714 0.36660215 0.39120374]\n",
      "  [0.45125288 0.45096381 0.45046236 ... 0.39000714 0.36660215 0.39120374]\n",
      "  [0.45125288 0.45096381 0.45046236 ... 0.39000714 0.36660215 0.39120374]]\n",
      "\n",
      " [[0.41147389 0.41113807 0.41057092 ... 0.34566349 0.32740187 0.34814338]\n",
      "  [0.41147389 0.41113807 0.41057092 ... 0.34566349 0.32740187 0.34814338]\n",
      "  [0.41147389 0.41113807 0.41057092 ... 0.34566349 0.32740187 0.34814338]\n",
      "  ...\n",
      "  [0.41147389 0.41113807 0.41057092 ... 0.34566349 0.32740187 0.34814338]\n",
      "  [0.41147389 0.41113807 0.41057092 ... 0.34566349 0.32740187 0.34814338]\n",
      "  [0.41147389 0.41113807 0.41057092 ... 0.34566349 0.32740187 0.34814338]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.05408832 0.05369737 0.05325814 ... 0.03402282 0.03457724 0.03430819]\n",
      "  [0.05408832 0.05369737 0.05325814 ... 0.03402282 0.03457724 0.03430819]\n",
      "  [0.05408832 0.05369737 0.05325814 ... 0.03402282 0.03457724 0.03430819]\n",
      "  ...\n",
      "  [0.05408832 0.05369737 0.05325814 ... 0.03402282 0.03457724 0.03430819]\n",
      "  [0.05408832 0.05369737 0.05325814 ... 0.03402282 0.03457724 0.03430819]\n",
      "  [0.05408832 0.05369737 0.05325814 ... 0.03402282 0.03457724 0.03430819]]\n",
      "\n",
      " [[0.02667114 0.02645601 0.02621909 ... 0.01633792 0.01668862 0.01645408]\n",
      "  [0.02667114 0.02645601 0.02621909 ... 0.01633792 0.01668862 0.01645408]\n",
      "  [0.02667114 0.02645601 0.02621909 ... 0.01633792 0.01668862 0.01645408]\n",
      "  ...\n",
      "  [0.02667114 0.02645601 0.02621909 ... 0.01633792 0.01668862 0.01645408]\n",
      "  [0.02667114 0.02645601 0.02621909 ... 0.01633792 0.01668862 0.01645408]\n",
      "  [0.02667114 0.02645601 0.02621909 ... 0.01633792 0.01668862 0.01645408]]\n",
      "\n",
      " [[0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]\n",
      "  [0.         0.         0.         ... 0.         0.         0.        ]]], shape=(30, 24, 59), dtype=float64) tf.Tensor(\n",
      "[-163.230127 -165.591702 -167.200859 -168.147552 -168.529501 -168.429148\n",
      " -167.91796  -167.058893 -165.9076   -164.513138 -162.918559 -161.161481\n",
      " -159.2747   -157.285683 -155.214666 -161.088015 -163.451499 -165.071919\n",
      " -166.030385 -166.424518 -166.337045 -165.839453 -164.994614 -163.858055\n",
      " -162.478697 -160.899427 -159.15764  -157.285775 -155.31185  -153.259515], shape=(30,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "#train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "#for element in train_dataset:\n",
    "#    print(element)\n",
    "for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "    print(step, x_batch_train, y_batch_train)\n",
    "    \n",
    "#list(train_dataset.as_numpy_iterator())[0].shape\n",
    "#x_tensor = tf.convert_to_tensor(x_train, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creat DNN without Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_type = 'float64'\n",
    "\n",
    "class tf_dnn(object):\n",
    "    '''\n",
    "    This class contains code for a simple feed forward neural network\n",
    "    '''\n",
    "    def __init__(self, n_layers):\n",
    "        '''\n",
    "        constructor\n",
    "        :param n_layers: number of nodes in each layer of the network\n",
    "        '''\n",
    "        # store the parameters of network\n",
    "        self.params = []\n",
    "\n",
    "        # Declare layer-wise weights and biases\n",
    "        # self.W1 = tf.Variable(tf.random.normal([n_layers[0], n_layers[1]], stddev=0.1,dtype=tf.dtypes.float64),name='W1')\n",
    "        # self.b1 = tf.Variable(tf.random.normal([n_layers[1]], mean=0.0, stddev=0.1, dtype=tf.dtypes.float32, seed=0), name='b1')\n",
    "        self.W1 = tf.Variable(tf.ones([n_layers[0], n_layers[1]],dtype=float_type))\n",
    "        self.b1 = tf.Variable(tf.zeros([1, n_layers[1]],dtype=float_type))\n",
    "\n",
    "        #self.W2 = tf.Variable(tf.random.normal([n_layers[1], n_layers[2]], stddev=0.1,dtype=tf.dtypes.float64),name='W2')\n",
    "        # self.b2 = tf.Variable(tf.random.normal([n_layers[2]], mean=0.0, stddev=0.1, dtype=tf.dtypes.float32, seed=0), name='b2')\n",
    "        self.W2 = tf.Variable(tf.ones([n_layers[1], n_layers[2]],dtype=float_type))\n",
    "        self.b2 = tf.Variable(tf.zeros([1, n_layers[2]],dtype=float_type))\n",
    "\n",
    "        #self.W3 = tf.Variable(tf.random.normal([n_layers[2], n_layers[3]],stddev=0.1,dtype=tf.dtypes.float64),name='W3')\n",
    "        # self.b3 = tf.Variable(tf.random.normal([n_layers[3]], mean=0.0, stddev=0.1, dtype=tf.dtypes.float32, seed=0), name='b3')\n",
    "        self.W3 = tf.Variable(tf.ones([n_layers[2], n_layers[3]],dtype=float_type))\n",
    "        self.b3 = tf.Variable(tf.zeros([1, n_layers[3]],dtype=float_type))\n",
    "\n",
    "        # Collect all initialized weights and biases in self.params\n",
    "        self.params = [self.W1, self.b1, self.W2, self.b2, self.W3, self.b3]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the network\n",
    "        :param x: input data\n",
    "        :return: predicted label\n",
    "        '''\n",
    "        #X_tf = tf.cast(x, dtype=tf.float32)\n",
    "        Z1 = tf.matmul(x, self.W1) + self.b1\n",
    "        Z1 = tf.nn.tanh(Z1)\n",
    "        Z2 = tf.matmul(Z1, self.W2) + self.b2\n",
    "        Z2 = tf.nn.tanh(Z2)\n",
    "        Z3 = tf.matmul(Z2, self.W3) + self.b3\n",
    "        Y = tf.reshape(tf.math.reduce_sum(Z3, axis=1),[Z3.get_shape().as_list()[0],])\n",
    "        return Y\n",
    "\n",
    "    def loss(self, y_true , y_pred):\n",
    "        '''\n",
    "        logits - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''\n",
    "        #y_true_tf = tf.cast(tf.reshape(y_true, (-1, 1)), dtype=tf.float32)\n",
    "        #logits_tf = tf.cast(tf.reshape(y_pred, (-1, 1)), dtype=tf.float32)\n",
    "        #return tf.compat.v1.losses.sigmoid_cross_entropy(y_true_tf, logits_tf)\n",
    "        #return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "        return tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "#     def train_step(self, x,y):\n",
    "#         #optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "#         #optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             predicted = self.forward(x)\n",
    "#             train_loss = self.loss(y, predicted)\n",
    "#         grads = tape.gradient(train_loss, self.params)\n",
    "#         print('parameters:',self.params)\n",
    "#         print ('loss gradient wrt weights:', grads)\n",
    "#         optimizer.apply_gradients(zip(grads, self.params))\n",
    "#         return train_loss\n",
    "    \n",
    "    def train_step(self, x,y):\n",
    "        #optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "        #optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(x)\n",
    "            predicted = self.forward(x)\n",
    "            train_loss = self.loss(y, predicted)\n",
    "        grads = tape.gradient(train_loss, self.params)\n",
    "        grads_input = tape.gradient(predicted, x)\n",
    "        grads_input_norm = tf.norm(grads_input)\n",
    "        optimizer.apply_gradients(zip(grads, self.params))\n",
    "        #print ('loss gradient wrt weights:', grads)\n",
    "        print ('output gradient wrt inputs:', grads_input_norm)\n",
    "        return train_loss\n",
    "    \n",
    "#     def gradient(self, x):\n",
    "#         #x_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "#         with tf.GradientTape() as t:\n",
    "#             t.watch(x)\n",
    "#             predicted = self.forward(x)\n",
    "#         grads_input = t.gradient(predicted, x).numpy()\n",
    "#         return grads_input\n",
    "        \n",
    "    def test_step(self, x, y):\n",
    "        x = (x - minfp)/(maxfp - minfp)\n",
    "        val_predicted = self.forward(x)\n",
    "        val_loss = self.loss(y, val_predicted)\n",
    "        return val_loss\n",
    "        #val_acc_metric.update_state(y, val_logits)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = [59,50,50,1]\n",
    "\n",
    "energy_nn = tf_dnn(n_layers = n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 0: Training loss: [for one batch: 83.7210; Running average: 83.7210]\n",
      "Seen so far: 30 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 1: Training loss: [for one batch: 212.7491; Running average: 148.2350]\n",
      "Seen so far: 60 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 2: Training loss: [for one batch: 80.3093; Running average: 125.5931]\n",
      "Seen so far: 90 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 3: Training loss: [for one batch: 212.5140; Running average: 147.3233]\n",
      "Seen so far: 120 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 4: Training loss: [for one batch: 83.4799; Running average: 134.5547]\n",
      "Seen so far: 150 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 5: Training loss: [for one batch: 207.0688; Running average: 146.6403]\n",
      "Seen so far: 180 samples\n",
      "output gradient wrt inputs: tf.Tensor(8409.6770999209, shape=(), dtype=float64)\n",
      "Batch 6: Training loss: [for one batch: 96.7663; Running average: 139.5155]\n",
      "Seen so far: 210 samples\n",
      "Validation loss: 172.8435\n",
      "Time taken: 0.13s\n",
      "\n",
      "Start of epoch 2\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 0: Training loss: [for one batch: 206.9912; Running average: 206.9912]\n",
      "Seen so far: 30 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 1: Training loss: [for one batch: 84.0587; Running average: 145.5250]\n",
      "Seen so far: 60 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 2: Training loss: [for one batch: 210.4029; Running average: 167.1509]\n",
      "Seen so far: 90 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 3: Training loss: [for one batch: 84.2938; Running average: 146.4366]\n",
      "Seen so far: 120 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 4: Training loss: [for one batch: 207.2324; Running average: 158.5958]\n",
      "Seen so far: 150 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 5: Training loss: [for one batch: 89.7390; Running average: 147.1196]\n",
      "Seen so far: 180 samples\n",
      "output gradient wrt inputs: tf.Tensor(2403.694319557098, shape=(), dtype=float64)\n",
      "Batch 6: Training loss: [for one batch: 199.5411; Running average: 154.6084]\n",
      "Seen so far: 210 samples\n",
      "Validation loss: 135.9911\n",
      "Time taken: 0.12s\n",
      "\n",
      "Start of epoch 3\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 0: Training loss: [for one batch: 83.9278; Running average: 83.9278]\n",
      "Seen so far: 30 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 1: Training loss: [for one batch: 212.5423; Running average: 148.2350]\n",
      "Seen so far: 60 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 2: Training loss: [for one batch: 80.5161; Running average: 125.6621]\n",
      "Seen so far: 90 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 3: Training loss: [for one batch: 212.3072; Running average: 147.3233]\n",
      "Seen so far: 120 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 4: Training loss: [for one batch: 83.6866; Running average: 134.5960]\n",
      "Seen so far: 150 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 5: Training loss: [for one batch: 206.8620; Running average: 146.6403]\n",
      "Seen so far: 180 samples\n",
      "output gradient wrt inputs: tf.Tensor(15444.041315982548, shape=(), dtype=float64)\n",
      "Batch 6: Training loss: [for one batch: 88.9368; Running average: 138.3970]\n",
      "Seen so far: 210 samples\n",
      "Validation loss: 165.9683\n",
      "Time taken: 0.13s\n",
      "\n",
      "Start of epoch 4\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 0: Training loss: [for one batch: 196.4985; Running average: 196.4985]\n",
      "Seen so far: 30 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 1: Training loss: [for one batch: 94.5514; Running average: 145.5250]\n",
      "Seen so far: 60 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 2: Training loss: [for one batch: 199.9102; Running average: 163.6534]\n",
      "Seen so far: 90 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 3: Training loss: [for one batch: 94.7865; Running average: 146.4366]\n",
      "Seen so far: 120 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 4: Training loss: [for one batch: 196.7396; Running average: 156.4972]\n",
      "Seen so far: 150 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 5: Training loss: [for one batch: 100.2317; Running average: 147.1196]\n",
      "Seen so far: 180 samples\n",
      "output gradient wrt inputs: tf.Tensor(10.837148867851903, shape=(), dtype=float64)\n",
      "Batch 6: Training loss: [for one batch: 189.6460; Running average: 153.1948]\n",
      "Seen so far: 210 samples\n",
      "Validation loss: 150.4624\n",
      "Time taken: 0.13s\n",
      "\n",
      "Start of epoch 5\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 0: Training loss: [for one batch: 97.2483; Running average: 97.2483]\n",
      "Seen so far: 30 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 1: Training loss: [for one batch: 199.2218; Running average: 148.2350]\n",
      "Seen so far: 60 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 2: Training loss: [for one batch: 93.8366; Running average: 130.1022]\n",
      "Seen so far: 90 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 3: Training loss: [for one batch: 198.9867; Running average: 147.3233]\n",
      "Seen so far: 120 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 4: Training loss: [for one batch: 97.0072; Running average: 137.2601]\n",
      "Seen so far: 150 samples\n",
      "output gradient wrt inputs: tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "Batch 5: Training loss: [for one batch: 193.5415; Running average: 146.6403]\n",
      "Seen so far: 180 samples\n",
      "output gradient wrt inputs: tf.Tensor(50.939463022607946, shape=(), dtype=float64)\n",
      "Batch 6: Training loss: [for one batch: 104.0876; Running average: 140.5614]\n",
      "Seen so far: 210 samples\n",
      "Validation loss: 165.9682\n",
      "Time taken: 0.13s\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    total_train_loss = 0\n",
    "    ave_train_loss = 0\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        train_loss = energy_nn.train_step(x_batch_train, y_batch_train)\n",
    "        total_train_loss = train_loss + total_train_loss       \n",
    "        ave_train_loss = total_train_loss/(step+1)\n",
    "        \n",
    "        \n",
    "        # Log every log_step batches.\n",
    "        log_step = 1\n",
    "        if step % log_step == 0:\n",
    "            print(\n",
    "                \"Batch %d: Training loss: [for one batch: %.4f; Running average: %.4f]\"\n",
    "                % (step, float(train_loss),float(ave_train_loss))\n",
    "            )\n",
    "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "\n",
    "    #ave_train_loss = ave_train_loss/(step+1)\n",
    "    #print(\"Average Training loss: %.4f\"% (float(ave_train_loss),))\n",
    "    \n",
    "    \n",
    "    # Display metrics at the end of each epoch.\n",
    "    #train_acc = train_acc_metric.result()\n",
    "    #print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    #train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    total_val_loss = 0\n",
    "    for step,(x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
    "        val_loss = energy_nn.test_step(x_batch_val, y_batch_val)\n",
    "        total_val_loss += val_loss\n",
    "    ave_val_loss = total_val_loss/(step+1)\n",
    "\n",
    "    #val_acc = val_acc_metric.result()\n",
    "    #val_acc_metric.reset_states()\n",
    "    #print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "    print(\"Validation loss: %.4f\" % (float(ave_val_loss),))\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "    \n",
    "    #if (total_val_loss<0.5):\n",
    "    #    break\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras DNN model (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_number=[5,5]\n",
    "input={}\n",
    "atom_energy_layer={}\n",
    "hidden_layer={}\n",
    "activation_function=\"tanh\"\n",
    "\n",
    "zero_initializer = tf.keras.initializers.Zeros()\n",
    "one_initializer = tf.keras.initializers.Ones()\n",
    "\n",
    "#ones = one_initializer(shape=(3,3),dtype=float_type)\n",
    "\n",
    "for i in range(len(element)):\n",
    "    input[i] = keras.layers.Input(shape=[None,fingerprint_num], name=\"element_\"+element[i]+\"_input\")\n",
    "    hidden_layer[i]={}\n",
    "    hidden_layer[i][0] = keras.layers.Dense(neuron_number[0], kernel_initializer=one_initializer, bias_initializer=zero_initializer, activation=activation_function,name=\"element_\"+element[i]+\"_hidden1\")(input[i])\n",
    "    for j in range(1,len(neuron_number)):\n",
    "        hidden_layer[i][j] = keras.layers.Dense(neuron_number[j], kernel_initializer=one_initializer, bias_initializer=zero_initializer,activation=activation_function,name=\"element_\"+element[i]+\"_hidden\"+str(j+1))(hidden_layer[i][j-1])\n",
    "        \n",
    "    atom_energy_layer[i] = keras.layers.Dense(1, kernel_initializer=one_initializer, bias_initializer=zero_initializer,activation=\"linear\",name=\"element_\"+element[i]+\"_atom_energy\")(hidden_layer[i][len(neuron_number)-1])\n",
    "\n",
    "concat_list=[]\n",
    "input_list=[]\n",
    "for i in range(len(element)):\n",
    "    concat_list.append(atom_energy_layer[i])\n",
    "    input_list.append(input[i])\n",
    "    \n",
    "if len(element)>1:\n",
    "    concat = keras.layers.concatenate(concat_list,axis=1,name=\"concat_atom_energy\") \n",
    "    output = keras.layers.Lambda(lambda x: K.sum(x, axis=1))(concat)\n",
    "else:\n",
    "    output = keras.layers.Lambda(lambda x: K.sum(x, axis=1))(concat_list[0])\n",
    "    \n",
    "model = keras.Model(inputs=input_list, outputs=output)\n",
    "#model=keras.models.load_model('amp_keras.dnn')\n",
    "#optimizer = keras.optimizers.SGD(lr=0.001)\n",
    "\n",
    "#opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "\n",
    "#optimizer='adam'\n",
    "loss_fun = tf.keras.losses.MeanAbsoluteError()\n",
    "model.compile(loss=loss_fun, optimizer=opt)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Callback=keras.callbacks.Callback\n",
    "\n",
    "class EarlyStoppingByLossVal(Callback):\n",
    "    def __init__(self, monitor='val_loss', value=0.00001, verbose=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         current = logs.get(self.monitor)\n",
    "#         if current is None:\n",
    "#             warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "#         if current < self.value:\n",
    "#             if self.verbose > 0:\n",
    "#                 print(\"Epoch %05d: early stopping THR\" % epoch)\n",
    "#             self.model.stop_training = True\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        print(\"\\nBatch %d: loss %.4f\" % (batch, logs['loss']))\n",
    "\n",
    "checkpoint_cb=keras.callbacks.ModelCheckpoint(\"amp_keras.dnn\",monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStoppingByLossVal(monitor='val_loss', value=0.5, verbose=1),\n",
    " #   checkpoint_cb\n",
    "    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),\n",
    "    #ModelCheckpoint(kfold_weights_path, monitor='loss', save_best_only=True, verbose=0),\n",
    "]\n",
    "#history = model.fit((X_train_A, X_train_B), y_train, batch_size=1, nb_epoch=50000,callbacks=callbacks)\n",
    "\n",
    "#history = model.fit((X_train_A, X_train_B), y_train, epochs=500,batch_size=1)\n",
    "history = model.fit(x_train, y_train, epochs=5,batch_size=30,verbose=2, shuffle=False, validation_data=(x_valid, y_valid),callbacks=callbacks)\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
