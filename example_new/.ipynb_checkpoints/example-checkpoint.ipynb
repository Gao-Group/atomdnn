{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates the whole process from initial atomic structure to training, evaluation and prediction. It includes:\n",
    "\n",
    "\n",
    "1. Read input atomic structures (saved as extxyz files) and create descriptors and their derivatives.\n",
    "\n",
    "2. Read inputs and outputs into a Data object.\n",
    "\n",
    "3. Create tensorflow dataset for training.\n",
    "\n",
    "4. Train the potential and apply it for prediction.\n",
    "\n",
    "5. Save the trained model and then load it for retraining or prediction.\n",
    "\n",
    "\n",
    "The code has been tested on Tensorflow 2.5 and 2.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atomdnn\n",
    "\n",
    "# 'float64' is used for reading data and train by default\n",
    "atomdnn.data_type = 'float64'\n",
    "\n",
    "# force and stress are evaluated by default, \n",
    "# if one only need to compute potential energy, then set compute_force to false\n",
    "atomdnn.compute_force = True\n",
    "\n",
    "# default value is for converting ev/A^3 to GPa\n",
    "# note that: the predicted positive stress means tension and negative stress means compression\n",
    "stress_unit_convert = 160.2176 \n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from atomdnn import data\n",
    "from atomdnn.data import Data\n",
    "from atomdnn.data import *\n",
    "from atomdnn.descriptor import *\n",
    "from atomdnn import network\n",
    "from atomdnn.network import Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read input atomic structures (saved as extxyz files) and create descriptors and their derivatives**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of fingerprints = 14\n"
     ]
    }
   ],
   "source": [
    "descriptor = {'name': 'acsf', \n",
    "              'cutoff': 6.5,\n",
    "              'etaG2':[0.01,0.1,1,5,10], \n",
    "              'etaG4': [0.01], \n",
    "              'zeta': [0.08,1.0,10.0,100.0],\n",
    "              'lambda': [1.0, -1.0]}\n",
    "\n",
    "\n",
    "# define lammps excutable (serial or mpi) \n",
    "# LAMMPS has to be compiled with the added compute and dump_local subrutines (inside atomdnn/lammps)\n",
    "lmpexe = 'lmp_serial' \n",
    "#lmpexe = 'mpirun -np 2 lmp_mpi' \n",
    "\n",
    "# elements = ['Mo','Te']\n",
    "# xyzfile_path = './extxyz_mote2' \n",
    "# xyzfile_name = 'XYZ_*'\n",
    "# descriptors_path = './descriptors_mote2'\n",
    "\n",
    "elements = ['C']\n",
    "xyzfile_path='extxyz_graphene_different_natoms'\n",
    "xyzfile_name = '24atoms.*' # a serials of files like example_extxyz.1, example_extxyz.2, ...example_extxyz.n\n",
    "descriptors_path = './descriptors_graphene'\n",
    "\n",
    "descriptor_filename = 'dump_fp_*' # a serials of dump_fp.* files will be created\n",
    "der_filename ='dump_der_*'\n",
    "\n",
    "print('total number of fingerprints = %i'%get_num_fingerprints(descriptor,elements))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are existing files in ./descriptors_graphene, do you want to first delete the files, y/n? n\n",
      "Start creating fingerprints and derivatives for 10 files ...\n",
      "  file-1: read atoms from '24atoms.1' and created descriptors in 'dump_fp_11' and derivatives in 'dump_der_11'\n",
      "  file-2: read atoms from '24atoms.2' and created descriptors in 'dump_fp_12' and derivatives in 'dump_der_12'\n",
      "  file-3: read atoms from '24atoms.3' and created descriptors in 'dump_fp_13' and derivatives in 'dump_der_13'\n",
      "  file-4: read atoms from '24atoms.4' and created descriptors in 'dump_fp_14' and derivatives in 'dump_der_14'\n",
      "  file-5: read atoms from '24atoms.5' and created descriptors in 'dump_fp_15' and derivatives in 'dump_der_15'\n",
      "  file-6: read atoms from '24atoms.6' and created descriptors in 'dump_fp_16' and derivatives in 'dump_der_16'\n",
      "  file-7: read atoms from '24atoms.7' and created descriptors in 'dump_fp_17' and derivatives in 'dump_der_17'\n",
      "  file-8: read atoms from '24atoms.8' and created descriptors in 'dump_fp_18' and derivatives in 'dump_der_18'\n",
      "  file-9: read atoms from '24atoms.9' and created descriptors in 'dump_fp_19' and derivatives in 'dump_der_19'\n",
      "  file-10: read atoms from '24atoms.10' and created descriptors in 'dump_fp_20' and derivatives in 'dump_der_20'\n",
      "  so far finished for 10 images ...\n",
      "Finish creating descriptors and derivatives for total 10 images.\n",
      "It took 3.66 seconds.\n"
     ]
    }
   ],
   "source": [
    "# this will create a serials of files for descriptors and their derivatives inside descriptors_path\n",
    "# by default, descriptor files are saved as 'dump_fp.*' and derivatives are saved as 'dump_der.*'\n",
    "create_descriptors(lmpexe,\n",
    "                   elements,\n",
    "                   xyzfile_path, \n",
    "                   xyzfile_name, \n",
    "                   descriptors_path, \n",
    "                   descriptor, \n",
    "                   descriptor_filename, \n",
    "                   der_filename, \n",
    "                   start_file_id = 11,\n",
    "                   image_num = 10, \n",
    "                   skip = 0,\n",
    "                   keep_lmpfiles = True,\n",
    "                   verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read inputs&outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read inputs and outputs into a Data object** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Data object\n",
    "mydata = Data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata.natoms_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mWarning: data is not appendable, append has been set to False.\u001b[0m\n",
      "Start reading fingerprints from 'dump_fp_*' for total 20 files ...\n",
      "  file-1: read fingerprints from 'dump_fp_1'.\n",
      "  file-2: read fingerprints from 'dump_fp_2'.\n",
      "  file-3: read fingerprints from 'dump_fp_3'.\n",
      "  file-4: read fingerprints from 'dump_fp_4'.\n",
      "  file-5: read fingerprints from 'dump_fp_5'.\n",
      "  file-6: read fingerprints from 'dump_fp_6'.\n",
      "  file-7: read fingerprints from 'dump_fp_7'.\n",
      "  file-8: read fingerprints from 'dump_fp_8'.\n",
      "  file-9: read fingerprints from 'dump_fp_9'.\n",
      "  file-10: read fingerprints from 'dump_fp_10'.\n",
      "  file-11: read fingerprints from 'dump_fp_11'.\n",
      "  file-12: read fingerprints from 'dump_fp_12'.\n",
      "  file-13: read fingerprints from 'dump_fp_13'.\n",
      "  file-14: read fingerprints from 'dump_fp_14'.\n",
      "  file-15: read fingerprints from 'dump_fp_15'.\n",
      "  file-16: read fingerprints from 'dump_fp_16'.\n",
      "  file-17: read fingerprints from 'dump_fp_17'.\n",
      "  file-18: read fingerprints from 'dump_fp_18'.\n",
      "  file-19: read fingerprints from 'dump_fp_19'.\n",
      "  file-20: read fingerprints from 'dump_fp_20'.\n",
      "  Finish reading fingerprints from total 20 images.\n",
      "\n",
      "\n",
      "Start reading derivatives from 'dump_der_*' for total 20 files ...\n",
      "  This may take a while for large data set ...\n",
      "  file-1: read derivatives from 'dump_der_1'.\n",
      "  file-2: read derivatives from 'dump_der_2'.\n",
      "  file-3: read derivatives from 'dump_der_3'.\n",
      "  file-4: read derivatives from 'dump_der_4'.\n",
      "  file-5: read derivatives from 'dump_der_5'.\n",
      "  file-6: read derivatives from 'dump_der_6'.\n",
      "  file-7: read derivatives from 'dump_der_7'.\n",
      "  file-8: read derivatives from 'dump_der_8'.\n",
      "  file-9: read derivatives from 'dump_der_9'.\n",
      "  file-10: read derivatives from 'dump_der_10'.\n",
      "  file-11: read derivatives from 'dump_der_11'.\n",
      "  file-12: read derivatives from 'dump_der_12'.\n",
      "  file-13: read derivatives from 'dump_der_13'.\n",
      "  file-14: read derivatives from 'dump_der_14'.\n",
      "  file-15: read derivatives from 'dump_der_15'.\n",
      "  file-16: read derivatives from 'dump_der_16'.\n",
      "  file-17: read derivatives from 'dump_der_17'.\n",
      "  file-18: read derivatives from 'dump_der_18'.\n",
      "  file-19: read derivatives from 'dump_der_19'.\n",
      "  file-20: read derivatives from 'dump_der_20'.\n",
      "  Finish reading dGdr derivatives from total 20 images.\n",
      "\n",
      "  It took 1.33 seconds to read the derivatives data.\n",
      "\n",
      "---------- input dataset information ----------\n",
      "total images = 20\n",
      "max number of atoms = 24\n",
      "number of fingerprints = 14\n",
      "number of atom types = 1\n",
      "max number of derivative pairs = 1244\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# read inputs: descriptors and their derivatives\n",
    "mydata.read_inputdata(descriptors_path, descriptor_filename, der_filename, \\\n",
    "                      image_num=20, skip=0, append=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading outputs from extxyz files ...\n",
      "  file-1: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.1\\`\n",
      "  file-2: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.2\\`\n",
      "  file-3: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.3\\`\n",
      "  file-4: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.4\\`\n",
      "  file-5: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.5\\`\n",
      "  file-6: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.6\\`\n",
      "  file-7: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.7\\`\n",
      "  file-8: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.8\\`\n",
      "  file-9: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.9\\`\n",
      "  file-10: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.10\\`\n",
      "  file-11: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.11\\`\n",
      "  file-12: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.12\\`\n",
      "  file-13: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.13\\`\n",
      "  file-14: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.14\\`\n",
      "  file-15: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.15\\`\n",
      "  file-16: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.16\\`\n",
      "  file-17: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.17\\`\n",
      "  file-18: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.18\\`\n",
      "  file-19: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.19\\`\n",
      "  file-20: read potential energy, forces and stress from \\`/mnt/GitHub/atomdnn-dev/example_new/extxyz_graphene_different_natoms/24atoms.20\\`\n",
      "  Finish reading outputs from total 20 images.\n",
      "\n",
      "\n",
      "---------- output dataset information ------------\n",
      "total images = 20\n",
      "max number of atoms = 24\n",
      "read_force = True\n",
      "read_stress = True\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# read outputs: potential energy, force and stress from extxyz files\n",
    "xyzfile_name = '4atoms.*'\n",
    "mydata.read_outputdata(xyzfile_path=xyzfile_path, xyzfile_name=xyzfile_name, \\\n",
    "                       image_num=20, skip=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mydata.input_dict['fingerprints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_Mo = mydata.input_dict['fingerprints'][0:10,0:18]\n",
    "x_Te = mydata.input_dict['fingerprints'][0:10,18:]\n",
    "x = [x_Mo,x_Te]\n",
    "y = mydata.output_dict['pe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TFdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create tensorflow dataset for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The atom numbers in fingerprints files and force files are not consistant.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-76824d487f51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# convert data to tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmydata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_data_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/Github/atomdnn-dev/atomdnn/data.py\u001b[0m in \u001b[0;36mconvert_data_to_tensor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_data_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \"\"\"\n\u001b[1;32m    496\u001b[0m         \u001b[0mConvert\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mouput\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mto\u001b[0m \u001b[0mTensorflow\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mspeed\u001b[0m \u001b[0mup\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mmanipulation\u001b[0m \u001b[0musing\u001b[0m \u001b[0mTensorflow\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/Github/atomdnn-dev/atomdnn/data.py\u001b[0m in \u001b[0;36mcheck_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No potential energy in output data.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m'force'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_images\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'force'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The image numbers of fingerprints files and force files are not consistant.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The atom numbers in fingerprints files and force files are not consistant."
     ]
    }
   ],
   "source": [
    "# convert data to tensors\n",
    "mydata.convert_data_to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensorflow dataset\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((mydata.input_dict,mydata.output_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = './graphene_tfdata'\n",
    "dataset_path = './mote2_tfdata'\n",
    "# save the dataset\n",
    "tf.data.experimental.save(tf_dataset, dataset_path)\n",
    "\n",
    "# save the element_spec to disk for future loading, this is only needed for tensorflow lower than 2.6\n",
    "with open(dataset_path + '/element_spec', 'wb') as out_: \n",
    "    pickle.dump(tf_dataset.element_spec, out_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The above three steps just need to be done once for one data set, the training only uses the saved tensorflow dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the dataset and train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tensorflow dataset, for Tensorflow version lower than 2.6, need to specify element_spec.\n",
    "\n",
    "with open(dataset_path + '/element_spec', 'rb') as in_:\n",
    "    element_spec = pickle.load(in_)\n",
    "\n",
    "dataset = tf.data.experimental.load(dataset_path,element_spec=element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to training, validation and testing sets\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset,1.0,0.0,0.0,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the network\n",
    "# See section 'Training' for detailed description on Network object.\n",
    "\n",
    "act_fun = 'relu' # activation function\n",
    "nfp = get_num_fingerprints(descriptor,elements) # number of fingerprints (or descriptors)\n",
    "arch = [10,10] # NN layers\n",
    "weights_init = tf.ones \n",
    "\n",
    "model = Network(elements, nfp, arch, act_fun, weights_initializer = weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.input_dict['volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "\n",
    "opt = 'Adam' # optimizer\n",
    "loss_fun = 'mae' # loss function\n",
    "scaling = None # scaling the traning data with standardization\n",
    "lr = 0.01 # learning rate\n",
    "loss_weights = {'pe' : 1, 'force' : 0, 'stress': 0} # the weights in loss function\n",
    "\n",
    "model.train(train_dataset,\\\n",
    "            optimizer=opt, \\\n",
    "            loss_fun = loss_fun, \\\n",
    "            batch_size=30, \\\n",
    "            lr=lr, \\\n",
    "            epochs=5, \\\n",
    "            scaling=scaling, \\\n",
    "            loss_weights=loss_weights, \\\n",
    "            compute_all_loss=True, \\\n",
    "            shuffle=False, \\\n",
    "            append_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "\n",
    "model.plot_loss(start_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using the first 5 data in test dataset\n",
    "\n",
    "model.evaluate(test_dataset.take(5),return_prediction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction using the first 5 data in test dataset\n",
    "\n",
    "input_dict = get_input_dict(dataset.take(1))\n",
    "model.predict(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**save the trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we re-write the descriptor here to empasize that it should be the same one defined above\n",
    "descriptor = {'name': 'acsf', \n",
    "              'cutoff': 6.5,\n",
    "              'etaG2':[0.01,0.05,0.1,0.5,1,5,10], \n",
    "              'etaG4': [0.01], \n",
    "              'zeta': [0.08,0.2,1.0,5.0,10.0,50.0,100.0],\n",
    "              'lambda': [1.0, -1.0]}\n",
    "\n",
    "save_dir = 'example.tfdnn'\n",
    "network.save(model,save_dir,descriptor=descriptor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the trained model for continuous training and prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_model = network.load(save_dir)\n",
    "\n",
    "# Re-train the model \n",
    "loss_weights = {'pe' : 1, 'force' : 1, 'stress': 0.1}\n",
    "\n",
    "opt = 'Adam'\n",
    "loss_fun = 'rmse'\n",
    "scaling = 'std'\n",
    "\n",
    "model.train(train_dataset, val_dataset, \n",
    "            optimizer=opt, \n",
    "            loss_fun = loss_fun, \n",
    "            batch_size=30, \n",
    "            lr=0.02, \n",
    "            epochs=5, \n",
    "            scaling=scaling, \n",
    "            loss_weights=loss_weights, \n",
    "            compute_all_loss=True, \n",
    "            shuffle=True, \n",
    "            append_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_model.evaluate(test_dataset.take(5),return_prediction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = get_input_dict(test_dataset.take(5))\n",
    "imported_model.predict(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras layer for energy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "fingerprint_num = get_num_fingerprints(descriptor,elements)\n",
    "\n",
    "zero_initializer = tf.keras.initializers.Zeros()\n",
    "one_initializer = tf.keras.initializers.Ones()\n",
    "\n",
    "# build keras DNN model \n",
    "neuron_number=[10,10]\n",
    "\n",
    "input={}\n",
    "atom_energy_layer={}\n",
    "hidden_layer={}\n",
    "activation_function=\"relu\"\n",
    "\n",
    "for i in range(len(elements)):\n",
    "    input[i] = keras.layers.Input(shape=[None,fingerprint_num], name=\"element_\"+elements[i]+\"_input\")\n",
    "    hidden_layer[i]={}\n",
    "    hidden_layer[i][0] = keras.layers.Dense(neuron_number[0], \n",
    "                                            kernel_initializer=one_initializer, \n",
    "                                            bias_initializer=zero_initializer, \n",
    "                                            activation=activation_function,\n",
    "                                            name=\"element_\"+elements[i]+\"_hidden1\")(input[i])\n",
    "    for j in range(1,len(neuron_number)):\n",
    "        hidden_layer[i][j] = keras.layers.Dense(neuron_number[j], \n",
    "                                                kernel_initializer=one_initializer, \n",
    "                                                bias_initializer=zero_initializer, \n",
    "                                                activation=activation_function,\n",
    "                                                name=\"element_\"+elements[i]+\"_hidden\"+str(j+1))(hidden_layer[i][j-1])\n",
    "        \n",
    "    atom_energy_layer[i] = keras.layers.Dense(1, kernel_initializer=one_initializer, \n",
    "                                              bias_initializer=zero_initializer, \n",
    "                                              activation=\"linear\",\n",
    "                                              name=\"element_\"+elements[i]+\"_atom_energy\")(hidden_layer[i][len(neuron_number)-1])\n",
    "\n",
    "concat_list=[]\n",
    "input_list=[]\n",
    "for i in range(len(elements)):\n",
    "    concat_list.append(atom_energy_layer[i])\n",
    "    input_list.append(input[i])\n",
    "    \n",
    "if len(elements)>1:\n",
    "    concat = keras.layers.concatenate(concat_list,axis=1,name=\"concat_atom_energy\") \n",
    "    output = keras.layers.Lambda(lambda x: K.sum(x, axis=1))(concat)\n",
    "else:\n",
    "    output = keras.layers.Lambda(lambda x: K.sum(x, axis=1))(concat_list[0])    \n",
    "\n",
    "model = keras.Model(inputs=input_list, outputs=output)\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "#opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "model.compile(loss=\"mae\", optimizer=opt)\n",
    "\n",
    "model.summary()\n",
    "model.predict(x)\n",
    "\n",
    "#history = model.fit(x_train_list, y_train, epochs=5,batch_size=1, validation_data=(x_valid_list, y_valid))\n",
    "\n",
    "history = model.fit(x, y, epochs=5,batch_size=30)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
